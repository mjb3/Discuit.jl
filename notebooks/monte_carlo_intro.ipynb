{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction to Monte Carlo methods\n",
    "## Sampling from discrete probability distributions\n",
    "#### Martin Burke, August 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo methods are a way of drawing samples from probability distributions (they are also used for problems such as optimization and computing integrals). In the examples below the (not so) difficult to sample distribution of interest is $X$: the outcome in a game of dice where the score $x = d_1 + d_2$ is the sum of two fair dice and the likelihood of any given score is denoted $f(x)$.\n",
    "\n",
    "Different sampling algorithms can be understood in terms of the challenges and requirements they are designed to accomodate. In some situations we may know enough about the system to draw directly (i.e. simulate) from $X$. In others we may be restricted to computing the probability mass function (PMF) $f(x)$; an unbiased approximation $\\hat{f}(x)$; or some quantity proportional to the likelihood $q \\propto f(x)$. We may also be interested in specific regions or scenarios concerning the target distribution such as the likelihood rolling 10 or more. More formally, we might wish to evaluate $\\int_{10}^{12} f(x) dx$.\n",
    "\n",
    "\n",
    "| Problem | Algorithms |\n",
    "|---------|-----------|\n",
    "| Draw directly from $X$  | Plain Monte Carlo |\n",
    "| Can only compute $f(x)$, $\\hat{f}(x)$ or $q \\propto f(x)$  | Rejection, Importance sampling |\n",
    "| Need to compute $\\int_{a}^{b} f(x) dx$ | Importance sampling |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plain Monte Carlo\n",
    "\n",
    "The first sampler is for situations where we know enough about the target distribution to draw samples directly. To do this we use our knowledge of the data generating process to define a function which draws two random numbers distributed uniformly from one to six and returns the sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# import some stuff for random number generation, analysis and plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotnine import *\n",
    "from plotnine.data import *\n",
    "\n",
    "\n",
    "# draw directly from X\n",
    "def dat_gen_function():\n",
    "    x = np.random.randint(1, 7)\n",
    "    y = np.random.randint(1, 7)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm itself is a simple one: it iteratively calls the function (passed as a parameter 'f') for a given number of steps. Each sample is appended to a list which is returned as a pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 1) monte carlo\n",
    "def plain_monte_carlo(steps, f):\n",
    "    # get some samples\n",
    "    samples = list()\n",
    "    for i in range(0, steps):\n",
    "        x = f()\n",
    "        # create a tuple and append to list\n",
    "        sample = (i, x)\n",
    "        samples.append(sample)\n",
    "    # create data frame and return\n",
    "    return pd.DataFrame(samples, columns=[\"i\", \"x\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the algorithm and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = plain_monte_carlo(200, dat_gen_function)\n",
    "p = ggplot(s, aes(\"x\")) + geom_histogram(binwidth=1)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that as the number of steps is increased the random noise in the distribution of samples obtained is reduced. That is, they converge upon the true target distribution: $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rejection sampling\n",
    "\n",
    "In situations where sampling from $X$ directly is difficult we may still be able to obtain samples if we are able to compute the PMF of the target distribution: $f(x)$ or an unbiased estimate. The rejection sampling method accomplishes this by drawing $x$ from some other easier to sample proposal distribution $G$ and accepting or rejecting the sample with probability: $$pr(accept) = \\frac{f(x)}{C g(x)}$$ where C is a constant chosen such that $f(x) < C g(x)$ for all $x$. Naturally since C is a constant this method works equally well in situations where it is more convenient to compute a quantity proportional to the likelihood. We will therefore begin by defining a function which returns the likelihood multiplied by an arbitrary \"unknown\" constant: $q(x)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arbitrary constant\n",
    "UC = 2\n",
    "\n",
    "\n",
    "# proportional quantity likelihood function\n",
    "def function_q_x(x):\n",
    "    comb = 6 - abs(x - 7)\n",
    "    return comb * 1/36 * UC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm is similar to the first but this time a likelihood function (or in this case $q(x)$) is passed as the function parameter. The proposal distribution $G$ is uniform between two and twelve. This is computationally convenient in the sense that we only have to evaluate $g(x)$ once but also somewhat wasteful in that many proposals are rejected. The choice of proposal distribution is therefore an important factor in algorithm efficiency but not one that we shall explore in detail here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 2) rejection sampler\n",
    "def rejection_sampler(steps, likelihood_function):\n",
    "    # compute C * g(x)\n",
    "    m_gx = 4 * 1/11     # C is chosen such that g(x) always > q(x)\n",
    "    # get some samples\n",
    "    samples = list()\n",
    "    for i in range(1, steps + 1):\n",
    "        # draw from proposal dist (uniform ~ 2, 12)\n",
    "        x = np.random.randint(2, 13)\n",
    "        # compute acceptance probability\n",
    "        pr_a = likelihood_function(x) / m_gx\n",
    "        # accept (or not)\n",
    "        if np.random.random() < pr_a:\n",
    "            sample = (i, x)\n",
    "            samples.append(sample)\n",
    "    # return as pandas data frame\n",
    "    return pd.DataFrame(samples, columns=[\"i\", \"x\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in contrast to the previous example each iteration of the algorithm involves two probabistic steps: the proposal itself and the additional step of accepting or rejecting with the computed probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rejection_sampler' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2af034739937>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrejection_sampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_q_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mggplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgeom_histogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rejection_sampler' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "s = rejection_sampler(10000, function_q_x)\n",
    "p = ggplot(s, aes(\"x\")) + geom_histogram(binwidth=1)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEED TO ADD acceptance rate commentary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance sampling\n",
    "\n",
    "The first two examples provided methods for drawing samples from $X$ in different situations. Importance sampling algorithms by contrast are designed to draw samples from the proposal density $G$ and weight them so as to recover information about $X$. First we consider the basic concept and then apply it to a slightly more difficult problem to understand why this might sometimes be useful.\n",
    "\n",
    "#### Basic example\n",
    "\n",
    "The proposal density used here is uniform from one to thirteen (i.e. we shall assume that we are not particularly good at choosing proposals):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1) importance sampling: basic concept\n",
    "def simple_importance_sampler(steps, likelihood_function):\n",
    "    # get some samples\n",
    "    samples = list()\n",
    "    for i in range(1, steps + 1):\n",
    "        # draw from proposal dist (uniform ~ 1, 13)\n",
    "        x = np.random.randint(1, 14)\n",
    "        # weight sample by target dist\n",
    "        # (since the g(x) is uniform we disregard for now)\n",
    "        w = likelihood_function(x)\n",
    "        sample = (i, x, w)\n",
    "        samples.append(sample)\n",
    "    # create data frame and return\n",
    "    return pd.DataFrame(samples, columns=[\"i\", \"x\", \"w\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the sampler using the likelihood function we already defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = simple_importance_sampler(10000, function_q_x)\n",
    "p = ggplot(s, aes(x = \"x\", y = \"..density..\", weight = \"w\")) + geom_histogram(binwidth=1)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Payoff example\n",
    "\n",
    "We now consider a slightly more complex problem to better illustrate the usefulness of importance sampling. Let $h(x)$ be a function that defines the payoff in a game of dice in a pretend casino. The player wins by rolling nine or more with the winnings being double the stake on nine; triple on ten and so on. The house wins if the player rolls seven or less and eight is a draw, i.e.the player keeps their stake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# payoff function\n",
    "def function_h_x(x):\n",
    "    if x < 8:\n",
    "        return 0\n",
    "    else:\n",
    "        return x - 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now interested in finding the expected value of the pay off. More formally, we wish to evaluate the following integral: $$\\int_{8}^{12} h(x) f(x) dx$$\n",
    "\n",
    "We shall also assume that we are in a position to compute an unbiased estimate of the full likelihood $\\hat{f}(x)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# likelihood function (not directly used)\n",
    "def correct_likelihood(x):\n",
    "    comb = 6 - abs(x - 7)\n",
    "    return comb * 1/36\n",
    "\n",
    "\n",
    "# likelihood estimator\n",
    "def dodgy_likelihood(x):\n",
    "    like = correct_likelihood(x)\n",
    "    pert = (np.random.random() - 0.5) * 0.1\n",
    "    return max(like + pert, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the updated weight calculation which accounts for the proposal density in addition to the estimated likelihood and the pay off function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2) importance sampling: pay off example\n",
    "def payoff_importance_sampler(steps, p1, p2, likelihood_function, pay_off_fn):\n",
    "    # get some samples\n",
    "    samples = list()\n",
    "    for i in range(1, steps + 1):\n",
    "        # draw from proposal dist (uniform ~ p1, p2)\n",
    "        x = np.random.randint(p1, p2 + 1)\n",
    "        # weight sample by f(x) / g(x) * h(x) \n",
    "        w = likelihood_function(x) * (p2 - p1 + 1) * pay_off_fn(x)\n",
    "        sample = (i, x, w)\n",
    "        samples.append(sample)\n",
    "    # create data frame and return\n",
    "    return pd.DataFrame(samples, columns=[\"i\", \"x\", \"w\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the proposal distribution $G$ is still uniform but is now parameterised. We begin by sampling using a better (but still not very good) proposal distribution, uniform on the range of possible outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = payoff_importance_sampler(1000, 2, 12, dodgy_likelihood, function_h_x)\n",
    "ev = s[\"w\"].mean()\n",
    "print(\"expected payoff: {}\".format(ev))\n",
    "p = ggplot(s, aes(x = \"x\", y = \"..density..\", weight = \"w\")) + geom_histogram(binwidth=1)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the algorithm we notice that samples obtained for proposals less than eight are essentially wasted since they do not contribute to the information we are able to recover about the expected payoff. We can therefore change the proposal density to only select from the desired range in order to improve the efficiency of the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = payoff_importance_sampler(1000, 8, 12, dodgy_likelihood, function_h_x)\n",
    "ev = s[\"w\"].mean()\n",
    "print(\"expected payoff: {}\".format(ev))\n",
    "p = ggplot(s, aes(x = \"x\", y = \"..density..\", weight = \"w\")) + geom_histogram(binwidth=1)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also test the performace gain by computing the average error overone hundred runs for a given number of samples from each proposal density:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_payoff = 35/36\n",
    "results = list()\n",
    "for i in range(0, 100):\n",
    "    s1 = payoff_importance_sampler(1000, 2, 12, dodgy_likelihood, function_h_x)\n",
    "    err1 = abs(true_payoff - s1[\"w\"].mean())\n",
    "    s2 = payoff_importance_sampler(1000, 8, 12, dodgy_likelihood, function_h_x)\n",
    "    err2 = abs(true_payoff - s2[\"w\"].mean())\n",
    "    results.append((err1, err2))\n",
    "results = pd.DataFrame(results, columns=[\"err1\", \"err2\"])\n",
    "print(\"First proposal average error: {}\".format(results[\"err1\"].mean()))\n",
    "print(\"Second proposal average error: {}\".format(results[\"err2\"].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly the average error is lower for the narrower proposal range. This demonstrates that importance sampling is a useful option for exploiring specific regions of the target distribution. This is a useful property with a range of applications from financial risk models to predicting the frequency of rare events in climate models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "This document introduced three basic classes of Monte Carlo algorithm in the context of drawing (independent) samples from a discrete probability distribution and touched upon the related problem of solving integrals. The next notebook introduces a more advanced class of Monte Carlo methods which draw future samples based on the current sample (i.e. dependent) to form a Markov chain (MCMC)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
