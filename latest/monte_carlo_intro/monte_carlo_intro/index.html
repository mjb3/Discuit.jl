<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="Martin Burke">
        
        <link rel="shortcut icon" href="../../img/favicon.ico">
        <title>Monte Carlo - Discuit.jl</title>
        <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../css/font-awesome-4.5.0.css" rel="stylesheet">
        <link href="../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="../../css/highlight.css">
        <link href="../../assets/Documenter.css" rel="stylesheet">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        <script src="../../js/jquery-1.10.2.min.js"></script>
        <script src="../../js/bootstrap-3.0.3.min.js"></script>
        <script src="../../js/highlight.pack.js"></script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="../..">Discuit.jl</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
                <!-- Main navigation -->
                <ul class="nav navbar-nav">
                    <li >
                        <a href="../..">Introduction</a>
                    </li>
                    <li >
                        <a href="../../examples/">Examples</a>
                    </li>
                    <li >
                        <a href="../../models/">Models</a>
                    </li>
                    <li >
                        <a href="../../manual/">Manual</a>
                    </li>
                    <li class="active">
                        <a href="./">Monte Carlo</a>
                    </li>
                    <li >
                        <a href="../../mcmc_intro/mcmc_intro/">Intro to MCMC</a>
                    </li>
                </ul>

            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                        <i class="fa fa-search"></i> Search
                    </a>
                </li>
                    <li >
                        <a rel="next" href="../../manual/">
                            <i class="fa fa-arrow-left"></i> Previous
                        </a>
                    </li>
                    <li >
                        <a rel="prev" href="../../mcmc_intro/mcmc_intro/">
                            Next <i class="fa fa-arrow-right"></i>
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/mjb3/Discuit.jl/edit/master/docs/monte_carlo_intro/monte_carlo_intro.md"><i class="fa fa-github"></i> Edit on GitHub</a>
                    </li>
            </ul>
        </div>
    </div>
</div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#introduction-to-monte-carlo-methods">Introduction to Monte Carlo methods</a></li>
            <li><a href="#sampling-from-discrete-probability-distributions">Sampling from discrete probability distributions</a></li>
            <li><a href="#summary">Summary</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<p><a id='Introduction-to-Monte-Carlo-methods-1'></a></p>
<h1 id="introduction-to-monte-carlo-methods">Introduction to Monte Carlo methods</h1>
<p><a id='Sampling-from-discrete-probability-distributions-1'></a></p>
<h2 id="sampling-from-discrete-probability-distributions">Sampling from discrete probability distributions</h2>
<p><a id='Martin-Burke,-August-2018-1'></a></p>
<h4 id="martin-burke-august-2018">Martin Burke, August 2018</h4>
<p>Monte Carlo methods are a way of drawing samples from probability distributions (they are also used for problems such as optimization and computing integrals). In the examples below the (not so) difficult to sample distribution of interest is $X$: the outcome in a game of dice where the score $x = d_1 + d_2$ is the sum of two fair dice and the likelihood of any given score is denoted $f(x)$.</p>
<p>Different sampling algorithms can be understood in terms of the challenges and requirements they are designed to accomodate. In some situations we may know enough about the system to draw directly (i.e. simulate) from $X$. In others we may be restricted to computing the probability mass function (PMF) $f(x)$; an unbiased approximation $\hat{f}(x)$; or some quantity proportional to the likelihood $q \propto f(x)$. We may also be interested in specific regions or scenarios concerning the target distribution such as the likelihood rolling 10 or more. More formally, we might wish to evaluate $\int_{10}^{12} f(x) dx$.</p>
<table>
<thead>
<tr>
<th align="right">Problem</th>
<th align="right">Algorithms</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right">Draw directly from $X$</td>
<td align="right">Plain Monte Carlo</td>
</tr>
<tr>
<td align="right" f="f">Can only compute $f(x)$, $\hat</td>
<td align="right">Rejection, Importance sampling</td>
</tr>
<tr>
<td a="a" align="right">Need to compute $\int_</td>
<td align="right">Importance sampling</td>
</tr>
</tbody>
</table>
<p><a id='Plain-Monte-Carlo-1'></a></p>
<h3 id="plain-monte-carlo">Plain Monte Carlo</h3>
<p>The first sampler is for situations where we know enough about the target distribution to draw samples directly. To do this we use our knowledge of the data generating process to define a function which draws two random numbers distributed uniformly from one to six and returns the sum:</p>
<pre class="codehilite"><code class="language-python"># import some stuff for random number generation, analysis and plotting
import numpy as np
import pandas as pd
from plotnine import *
from plotnine.data import *


# draw directly from X
def dat_gen_function():
    x = np.random.randint(1, 7)
    y = np.random.randint(1, 7)
    return x + y</code></pre>


<pre class="codehilite"><code>/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)</code></pre>


<p>The algorithm itself is a simple one: it iteratively calls the function (passed as a parameter 'f') for a given number of steps. Each sample is appended to a list which is returned as a pandas data frame.</p>
<pre class="codehilite"><code class="language-python"># example 1) monte carlo
def plain_monte_carlo(steps, f):
    # get some samples
    samples = list()
    for i in range(0, steps):
        x = f()
        # create a tuple and append to list
        sample = (i, x)
        samples.append(sample)
    # create data frame and return
    return pd.DataFrame(samples, columns=[&quot;i&quot;, &quot;x&quot;])</code></pre>


<p>We can now run the algorithm and plot the results.</p>
<pre class="codehilite"><code class="language-python">s = plain_monte_carlo(200, dat_gen_function)
p = ggplot(s, aes(&quot;x&quot;)) + geom_histogram(binwidth=1)
print(p)</code></pre>


<p><img alt="png" src="../output_7_0.png" /></p>
<pre class="codehilite"><code>&lt;ggplot: (-9223363295278400880)&gt;</code></pre>


<p>Note that as the number of steps is increased the random noise in the distribution of samples obtained is reduced. That is, they converge upon the true target distribution: $X$.</p>
<p><a id='Rejection-sampling-1'></a></p>
<h3 id="rejection-sampling">Rejection sampling</h3>
<p>In situations where sampling from $X$ directly is difficult we may still be able to obtain samples if we are able to compute the PMF of the target distribution: $f(x)$ or an unbiased estimate. The rejection sampling method accomplishes this by drawing $x$ from some other easier to sample proposal distribution $G$ and accepting or rejecting the sample with probability: $pr(accept) = \frac{f(x)}{C g(x)}$ where C is a constant chosen such that $f(x) &lt; C g(x)$ for all $x$. Naturally since C is a constant this method works equally well in situations where it is more convenient to compute a quantity proportional to the likelihood. We will therefore begin by defining a function which returns the likelihood multiplied by an arbitrary "unknown" constant: $q(x)$:</p>
<pre class="codehilite"><code class="language-python"># arbitrary constant
UC = 2


# proportional quantity likelihood function
def function_q_x(x):
    comb = 6 - abs(x - 7)
    return comb * 1/36 * UC</code></pre>


<p>The algorithm is similar to the first but this time a likelihood function (or in this case $q(x)$) is passed as the function parameter. The proposal distribution $G$ is uniform between two and twelve. This is computationally convenient in the sense that we only have to evaluate $g(x)$ once but also somewhat wasteful in that many proposals are rejected. The choice of proposal distribution is therefore an important factor in algorithm efficiency but not one that we shall explore in detail here.</p>
<pre class="codehilite"><code class="language-python"># example 2) rejection sampler
def rejection_sampler(steps, likelihood_function):
    # compute C * g(x)
    m_gx = 4 * 1/11     # C is chosen such that g(x) always &gt; q(x)
    # get some samples
    samples = list()
    for i in range(1, steps + 1):
        # draw from proposal dist (uniform ~ 2, 12)
        x = np.random.randint(2, 13)
        # compute acceptance probability
        pr_a = likelihood_function(x) / m_gx
        # accept (or not)
        if np.random.random() &lt; pr_a:
            sample = (i, x)
            samples.append(sample)
    # return as pandas data frame
    return pd.DataFrame(samples, columns=[&quot;i&quot;, &quot;x&quot;])</code></pre>


<p>Note that in contrast to the previous example each iteration of the algorithm involves two probabistic steps: the proposal itself and the additional step of accepting or rejecting with the computed probability.</p>
<pre class="codehilite"><code class="language-python">s = rejection_sampler(10000, function_q_x)
p = ggplot(s, aes(&quot;x&quot;)) + geom_histogram(binwidth=1)
print(p)</code></pre>


<p><img alt="png" src="../output_14_0.png" /></p>
<pre class="codehilite"><code>&lt;ggplot: (-9223363295278433683)&gt;</code></pre>


<p>NEED TO ADD acceptance rate commentary.</p>
<p><a id='Importance-sampling-1'></a></p>
<h3 id="importance-sampling">Importance sampling</h3>
<p>The first two examples provided methods for drawing samples from $X$ in different situations. Importance sampling algorithms by contrast are designed to draw samples from the proposal density $G$ and weight them so as to recover information about $X$. First we consider the basic concept and then apply it to a slightly more difficult problem to understand why this might sometimes be useful.</p>
<p><a id='Basic-example-1'></a></p>
<h4 id="basic-example">Basic example</h4>
<p>The proposal density used here is uniform from one to thirteen (i.e. we shall assume that we are not particularly good at choosing proposals):</p>
<pre class="codehilite"><code class="language-python"># 3.1) importance sampling: basic concept
def simple_importance_sampler(steps, likelihood_function):
    # get some samples
    samples = list()
    for i in range(1, steps + 1):
        # draw from proposal dist (uniform ~ 1, 13)
        x = np.random.randint(1, 14)
        # weight sample by target dist
        # (since the g(x) is uniform we disregard for now)
        w = likelihood_function(x)
        sample = (i, x, w)
        samples.append(sample)
    # create data frame and return
    return pd.DataFrame(samples, columns=[&quot;i&quot;, &quot;x&quot;, &quot;w&quot;])</code></pre>


<p>We can now run the sampler using the likelihood function we already defined:</p>
<pre class="codehilite"><code class="language-python">s = simple_importance_sampler(10000, function_q_x)
p = ggplot(s, aes(x = &quot;x&quot;, y = &quot;..density..&quot;, weight = &quot;w&quot;)) + geom_histogram(binwidth=1)
print(p)</code></pre>


<p><img alt="png" src="../output_19_0.png" /></p>
<pre class="codehilite"><code>&lt;ggplot: (-9223363295280596108)&gt;</code></pre>


<p><a id='Payoff-example-1'></a></p>
<h4 id="payoff-example">Payoff example</h4>
<p>We now consider a slightly more complex problem to better illustrate the usefulness of importance sampling. Let $h(x)$ be a function that defines the payoff in a game of dice in a pretend casino. The player wins by rolling nine or more with the winnings being double the stake on nine; triple on ten and so on. The house wins if the player rolls seven or less and eight is a draw, i.e.the player keeps their stake.</p>
<pre class="codehilite"><code class="language-python"># payoff function
def function_h_x(x):
    if x &lt; 8:
        return 0
    else:
        return x - 7</code></pre>


<p>We are now interested in finding the expected value of the pay off. More formally, we wish to evaluate the following integral: $\int_{8}^{12} h(x) f(x) dx$</p>
<p>We shall also assume that we are in a position to compute an unbiased estimate of the full likelihood $\hat{f}(x)$:</p>
<pre class="codehilite"><code class="language-python"># likelihood function (not directly used)
def correct_likelihood(x):
    comb = 6 - abs(x - 7)
    return comb * 1/36


# likelihood estimator
def dodgy_likelihood(x):
    like = correct_likelihood(x)
    pert = (np.random.random() - 0.5) * 0.1
    return max(like + pert, 0)</code></pre>


<p>Note the updated weight calculation which accounts for the proposal density in addition to the estimated likelihood and the pay off function:</p>
<pre class="codehilite"><code class="language-python"># 3.2) importance sampling: pay off example
def payoff_importance_sampler(steps, p1, p2, likelihood_function, pay_off_fn):
    # get some samples
    samples = list()
    for i in range(1, steps + 1):
        # draw from proposal dist (uniform ~ p1, p2)
        x = np.random.randint(p1, p2 + 1)
        # weight sample by f(x) / g(x) * h(x) 
        w = likelihood_function(x) * (p2 - p1 + 1) * pay_off_fn(x)
        sample = (i, x, w)
        samples.append(sample)
    # create data frame and return
    return pd.DataFrame(samples, columns=[&quot;i&quot;, &quot;x&quot;, &quot;w&quot;])</code></pre>


<p>Note that the proposal distribution $G$ is still uniform but is now parameterised. We begin by sampling using a better (but still not very good) proposal distribution, uniform on the range of possible outcomes.</p>
<pre class="codehilite"><code class="language-python">s = payoff_importance_sampler(1000, 2, 12, dodgy_likelihood, function_h_x)
ev = s[&quot;w&quot;].mean()
print(&quot;expected payoff: {}&quot;.format(ev))
p = ggplot(s, aes(x = &quot;x&quot;, y = &quot;..density..&quot;, weight = &quot;w&quot;)) + geom_histogram(binwidth=1)
print(p)</code></pre>


<pre class="codehilite"><code>expected payoff: 0.9385784069518784</code></pre>


<p><img alt="png" src="../output_27_1.png" /></p>
<pre class="codehilite"><code>&lt;ggplot: (-9223363295280591030)&gt;</code></pre>


<p>Running the algorithm we notice that samples obtained for proposals less than eight are essentially wasted since they do not contribute to the information we are able to recover about the expected payoff. We can therefore change the proposal density to only select from the desired range in order to improve the efficiency of the algorithm:</p>
<pre class="codehilite"><code class="language-python">s = payoff_importance_sampler(1000, 8, 12, dodgy_likelihood, function_h_x)
ev = s[&quot;w&quot;].mean()
print(&quot;expected payoff: {}&quot;.format(ev))
p = ggplot(s, aes(x = &quot;x&quot;, y = &quot;..density..&quot;, weight = &quot;w&quot;)) + geom_histogram(binwidth=1)
print(p)</code></pre>


<pre class="codehilite"><code>expected payoff: 0.9667801758779293</code></pre>


<p><img alt="png" src="../output_29_1.png" /></p>
<pre class="codehilite"><code>&lt;ggplot: (8741574212367)&gt;</code></pre>


<p>We can also test the performace gain by computing the average error overone hundred runs for a given number of samples from each proposal density:</p>
<pre class="codehilite"><code class="language-python">true_payoff = 35/36
results = list()
for i in range(0, 100):
    s1 = payoff_importance_sampler(1000, 2, 12, dodgy_likelihood, function_h_x)
    err1 = abs(true_payoff - s1[&quot;w&quot;].mean())
    s2 = payoff_importance_sampler(1000, 8, 12, dodgy_likelihood, function_h_x)
    err2 = abs(true_payoff - s2[&quot;w&quot;].mean())
    results.append((err1, err2))
results = pd.DataFrame(results, columns=[&quot;err1&quot;, &quot;err2&quot;])
print(&quot;First proposal average error: {}&quot;.format(results[&quot;err1&quot;].mean()))
print(&quot;Second proposal average error: {}&quot;.format(results[&quot;err2&quot;].mean()))</code></pre>


<pre class="codehilite"><code>First proposal average error: 0.0302439686410776
Second proposal average error: 0.018523710824803533</code></pre>


<p>Unsurprisingly the average error is lower for the narrower proposal range. This demonstrates that importance sampling is a useful option for exploiring specific regions of the target distribution. This is a useful property with a range of applications from financial risk models to predicting the frequency of rare events in climate models.</p>
<p><a id='Summary-1'></a></p>
<h2 id="summary">Summary</h2>
<p>This document introduced three basic classes of Monte Carlo algorithm in the context of drawing (independent) samples from a discrete probability distribution and touched upon the related problem of solving integrals. The next notebook introduces a more advanced class of Monte Carlo methods which draw future samples based on the current sample (i.e. dependent) to form a Markov chain (MCMC).</p></div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>var base_url = '../..';</script>
        <script src="../../js/base.js"></script>
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
        <script src="../../assets/mathjaxhelper.js"></script>
        <script src="https://cdn.rawgit.com/pcooksey/bibtex-js/ef59e62c/src/bibtex_js.js"></script>
        <script src="../../search/require.js"></script>
        <script src="../../search/search.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td><kbd>&larr;</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td><kbd>&rarr;</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>


    </body>
</html>
