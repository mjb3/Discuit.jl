{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Discuit.jl Fast parameter inference for discrete state space continuous time (DSSCT) models in Julia. Please note that this package is still in development. Discuit is a package for Bayesian inference in Discrete state space continuous time (DSSCT) models. DSSCT models, sometimes referred to as compartmental models, are used to represent systems where individuals are assumed, usually as a simplifying abstraction, to move between discrete states. They are a well established research tool in fields including physics, chemistry and ecology. Their use in scientific research typically involves the challenge of comparing \u2018noisy\u2019 experimental or observational data to the unobserved (i.e. latent) underlying processes described by the model. Bayes\u2019 theorem provides a convenient framework for dealing with uncertainty in the data. The Bayesian framework also provide a natural, methodologically consistent way of incorporating existing scientific knowledge of the system; the prior distribution of the model parameters The augmented data Markov chain Monte Carlo (MCMC) methods implemented in Discuit work by introducing a latent variable $\\xi$ to the model likelihood function which represents the sequence of events in a single realisation of the model: \\pi(\\theta|y) = \\pi(y|\\xi) \\pi(\\xi|\\theta) \\pi(\\theta) See Introduction to MCMC for a basic introduction to MCMC and Introduction to Monte Carlo methods for an overview of random sampling generally. Two algorithms for making proposals to the augmented data space are shipped with the package, with user defined implementations made possible via an alternative custom MCMC framework. Automated tools for analysis and convergence diagnostics include autocorrelation, the Geweke test of stationarity and the Gelman-Rubin diagnostic for multiple Markov chains (a convenient way to run analyses where more than one processor thread is available for use). Simulation via the Gillespie direct method provides a source of simulated observations data for evaluation and validation of the core inference functionality. See the Discuit.jl models section for a guide to the pre defined model library and the Discuit.jl manual for a description of types and functions. See the Discuit in R package documentation for a description of the equivalent functionality in that package. Package features # Discuit Module . Discuit is a package for: Customisable finite adaptive MCMC algorithm for fast parameter inference. Pooley model based proposal (MBP) method for improved mixing. Simulation via the Gillespie direct method. Automated tools for convergence diagnosis and analysis. Developed for Julia 1.0 . Author: Martin Burke (martin.burke@bioss.ac.uk) Date: 2018-08-22 source Installation The package can be installed by typing ] in the REPL to enter the Pkg mode and running: pkg add https://github.com/mjb3/Discuit.jl Getting started The following code initialises a DiscuitModel from the predefined library and runs a simulation, storing the results in x : julia using Discuit; julia model = generate_model( SIS , [100,1]); julia x = gillespie_sim(model, [0.003, 0.1]); running simulation... finished (1037 events). Demo: Note that the first simulation produced a trajectory with only three events. Hint: hitting up on the keyboard recalls previous commands for easy reuse. Having rerun the simulation (1067 on the second run) we can now run an MCMC analysis using observations data from x : julia mcmc = run_met_hastings_mcmc(model, x.observations, [0.005, 0.12]); running MCMC... finished (sample \u03bc = [0.0033532047996984297, 0.12130070161095252]) Demo: The MCMC output can also be visualised using the command line tool. Example: See the Discuit.jl examples page for code. Further information can be found in the Discuit.jl manual . Further usage More examples can be found in the section Discuit.jl examples , including enough code to get up and running with convergence diagnostics and customised models. A more detailed guide to the pre defined models is available in the Discuit.jl models section. Further information regarding the packages other functionality can be found in the Discuit.jl manual . Tutorials Introduction to Monte Carlo methods : a beginner's guide in Python. A basic Introduction to MCMC methods in Python. Discuit.jl examples : an introduction to MCMC and simulation in Discuit.jl for Julia. See the Discuit for R package documentation for R examples.","title":"Introduction"},{"location":"#discuitjl","text":"Fast parameter inference for discrete state space continuous time (DSSCT) models in Julia. Please note that this package is still in development. Discuit is a package for Bayesian inference in Discrete state space continuous time (DSSCT) models. DSSCT models, sometimes referred to as compartmental models, are used to represent systems where individuals are assumed, usually as a simplifying abstraction, to move between discrete states. They are a well established research tool in fields including physics, chemistry and ecology. Their use in scientific research typically involves the challenge of comparing \u2018noisy\u2019 experimental or observational data to the unobserved (i.e. latent) underlying processes described by the model. Bayes\u2019 theorem provides a convenient framework for dealing with uncertainty in the data. The Bayesian framework also provide a natural, methodologically consistent way of incorporating existing scientific knowledge of the system; the prior distribution of the model parameters The augmented data Markov chain Monte Carlo (MCMC) methods implemented in Discuit work by introducing a latent variable $\\xi$ to the model likelihood function which represents the sequence of events in a single realisation of the model: \\pi(\\theta|y) = \\pi(y|\\xi) \\pi(\\xi|\\theta) \\pi(\\theta) See Introduction to MCMC for a basic introduction to MCMC and Introduction to Monte Carlo methods for an overview of random sampling generally. Two algorithms for making proposals to the augmented data space are shipped with the package, with user defined implementations made possible via an alternative custom MCMC framework. Automated tools for analysis and convergence diagnostics include autocorrelation, the Geweke test of stationarity and the Gelman-Rubin diagnostic for multiple Markov chains (a convenient way to run analyses where more than one processor thread is available for use). Simulation via the Gillespie direct method provides a source of simulated observations data for evaluation and validation of the core inference functionality. See the Discuit.jl models section for a guide to the pre defined model library and the Discuit.jl manual for a description of types and functions. See the Discuit in R package documentation for a description of the equivalent functionality in that package.","title":"Discuit.jl"},{"location":"#package-features","text":"# Discuit Module . Discuit is a package for: Customisable finite adaptive MCMC algorithm for fast parameter inference. Pooley model based proposal (MBP) method for improved mixing. Simulation via the Gillespie direct method. Automated tools for convergence diagnosis and analysis. Developed for Julia 1.0 . Author: Martin Burke (martin.burke@bioss.ac.uk) Date: 2018-08-22 source","title":"Package features"},{"location":"#installation","text":"The package can be installed by typing ] in the REPL to enter the Pkg mode and running: pkg add https://github.com/mjb3/Discuit.jl","title":"Installation"},{"location":"#getting-started","text":"The following code initialises a DiscuitModel from the predefined library and runs a simulation, storing the results in x : julia using Discuit; julia model = generate_model( SIS , [100,1]); julia x = gillespie_sim(model, [0.003, 0.1]); running simulation... finished (1037 events). Demo: Note that the first simulation produced a trajectory with only three events. Hint: hitting up on the keyboard recalls previous commands for easy reuse. Having rerun the simulation (1067 on the second run) we can now run an MCMC analysis using observations data from x : julia mcmc = run_met_hastings_mcmc(model, x.observations, [0.005, 0.12]); running MCMC... finished (sample \u03bc = [0.0033532047996984297, 0.12130070161095252]) Demo: The MCMC output can also be visualised using the command line tool. Example: See the Discuit.jl examples page for code. Further information can be found in the Discuit.jl manual .","title":"Getting started"},{"location":"#further-usage","text":"More examples can be found in the section Discuit.jl examples , including enough code to get up and running with convergence diagnostics and customised models. A more detailed guide to the pre defined models is available in the Discuit.jl models section. Further information regarding the packages other functionality can be found in the Discuit.jl manual .","title":"Further usage"},{"location":"#tutorials","text":"Introduction to Monte Carlo methods : a beginner's guide in Python. A basic Introduction to MCMC methods in Python. Discuit.jl examples : an introduction to MCMC and simulation in Discuit.jl for Julia. See the Discuit for R package documentation for R examples.","title":"Tutorials"},{"location":"examples/","text":"Discuit.jl examples The following examples provide a flavour of package's core functionality. See the Discuit.jl manual for a description of the data types and functions in Discuit, and Discuit.jl models for a description of the predefined models available in the package. The tutorial is designed to be run using the REPL but usually it is wise to save code and analyses to a text file for reference later. By convention Julia code files have the extension '.jl'. For example, the file pooley_model.jl (click to download) contains code equivalent to the next section, i.e. it defines a SIS model and stores it in variable model which can then be used just as if we had typed the commands manually. One way to run a code file is to open a command prompt or terminal and cd to the location of the file. Check the code and save the file. Next start the REPL using the command julia . Finally type the name of the file to run the code: ADD GIF Defining a model DiscuitModel s can be created automatically using helper functions or manually by specifying each component. For example the model we are about to create could be generated automatically using generate_model(\"SIS\", [100,1]) . However constructing it manually is a helpful exercise for getting to know the package. See Discuit.jl models for further details of the generate_model function. We start by examining DiscuitModel in the package documentation: julia using Discuit; The events in a DiscuitModel are defined by the rates at which they occur and a transition matrix, which governs how individuals migrate between states. In the basic Kermack-McKendrick SIS (and SIR ) model, rates for infection and recovery events respectively are given by: r_1 = \\theta_1 SI r_2 = \\theta_2 I With the transition matrix: T = \\begin{bmatrix} -1 & 1 \\\\ 1 & -1 \\end{bmatrix} Note how the first row of the matrix reflects the removal of a susceptible individual from the S state, and migrates them to the second element of the row, the 'I' state. The code required to represent the rates (or 'rate function') and transition matrxi as Julia variables is correspondingly straightforward: julia function sis_rf(output::Array{Float64, 1}, parameters::Array{Float64, 1}, population::Array{Int64, 1}) output[1] = parameters[1] * population[1] * population[2] output[2] = parameters[2] * population[2] end sis_rf (generic function with 1 method) julia t_matrix = [-1 1; 1 -1] 2\u00d72 Array{Int64,2}: -1 1 1 -1 The output confirms that sis_rf , a generic function with 1 method has been defined and gives a description of the 2 dimensional Array variable that represents the transition matrix (do not copy and paste these bits when running the code on your own machine). Note that the correct function signature must be used in the implementation for it to be compatible with the package. In this case the function takes three Array parameters of a given type, the first of which is the output variable modified by the function (which is why it does not need to return any actual output variable). Next we define a simple observation function, again with the correct signature: julia obs_fn(population::Array{Int64, 1}) = population obs_fn (generic function with 1 method) The default prior distribution is flat and improper, and is equivalent to: julia function weak_prior(parameters::Array{Float64, 1}) parameters[1] 0.0 || return 0.0 parameters[2] 0.0 || return 0.0 return 1.0 end weak_prior (generic function with 1 method) Finally, we define an observation likelihood model. Again, we use the same as Pooley, with observation errors normally distributed around the true value with standard deviation 2 : julia function si_gaussian(y::Array{Int64, 1}, population::Array{Int64, 1}) obs_err = 2 tmp1 = log(1 / (sqrt(2 * pi) * obs_err)) tmp2 = 2 * obs_err * obs_err obs_diff = y[2] - population[2] return tmp1 - ((obs_diff * obs_diff) / tmp2) end si_gaussian (generic function with 1 method) We can now define a model. We must also specify the initial_condition which represents the state of the population at the origin of each trajectory. A final parameter declared inline is an optional index for the t0 parameter (ignore for now): julia initial_condition = [100, 1] 2-element Array{Int64,1}: 100 1 julia model = DiscuitModel( SIS , initial_condition, sis_rf, t_matrix, obs_fn, weak_prior, si_gaussian, 0); MCMC The following example is based on that published by Pooley et al. (2015) in the paper that introduces the model based proposal method. The observations data simulated by Pooley can be downloaded here and saved, e.g. to path/to/data/ . Next, load the observations data using: y = get_observations( path/to/data/pooley.csv ) Now we can run an MCMC analysis based on the simulated datset: julia rs = run_met_hastings_mcmc(model, y, [0.0025, 0.12]); running MCMC... finished (sample \u03bc = [0.0034241420925407253, 0.11487898025205769]) Visual inspection of the Markov chain using the traceplot is one way of assessing the convergence of the algorithm: julia plot_parameter_trace(rs, 1) The marginal distribution of parameters can be plotted by calling: julia plot_parameter_marginal(rs, 1) \u250c Warning: The keyword parameter `bins` is deprecated, use `nbins` instead \u2502 caller = ip:0x0 \u2514 @ Core :-1 \u250c \u2510 [0.001 , 0.0015) \u2524\u2587 295 [0.0015, 0.002 ) \u2524\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587 3378 [0.002 , 0.0025) \u2524\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587 6960 [0.0025, 0.003 ) \u2524\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587 7700 [0.003 , 0.0035) \u2524\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587 7265 [0.0035, 0.004 ) \u2524\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587 5425 [0.004 , 0.0045) \u2524\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587 3148 [0.0045, 0.005 ) \u2524\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587 2092 [0.005 , 0.0055) \u2524\u2587\u2587\u2587\u2587\u2587 1023 [0.0055, 0.006 ) \u2524\u2587\u2587\u2587 675 [0.006 , 0.0065) \u2524\u2587\u2587 494 \u03b8\u2081 [0.0065, 0.007 ) \u2524\u2587 271 [0.007 , 0.0075) \u2524 68 [0.0075, 0.008 ) \u2524\u2587 138 [0.008 , 0.0085) \u2524 76 [0.0085, 0.009 ) \u2524 80 [0.009 , 0.0095) \u2524 111 [0.0095, 0.01 ) \u2524\u2587 273 [0.01 , 0.0105) \u2524\u2587 195 [0.0105, 0.011 ) \u2524\u2587 133 [0.011 , 0.0115) \u2524 67 [0.0115, 0.012 ) \u2524 77 [0.012 , 0.0125) \u2524 57 \u2514 \u2518 samples Plotting the data with third party packages such as PyPlot is simple: using PyPlot x = mcmc.samples[mcmc.adapt_period:size(mcmc.samples, 1), parameter] plt[:hist](x, 30) xlabel(string( \\$\\\\theta_ , parameter, \\\\$ )) ylabel( density ) A pairwise representation can be produced by calling plot_parameter_heatmap (see below for an example). Autocorrelation Autocorrelation can be used to help determine how well the algorithm mixed by using compute_autocorrelation . The autocorrelation function for a single Markov chain is implemented in Discuit using the standard formula: R_l = \\frac{\\textrm{E} [(X_i - \\bar{X})(X_{i+l} - \\bar{X})]}{\\sigma^2} for any given lag l . The modified formula for multiple chains is given by: R_{b,l} = \\frac{\\textrm{E} [ (X_i - \\bar{X}_b) ( X_{i + l} - \\bar{X}_b ) ]}{\\sigma^2_b} \\sigma^2_b = \\textrm{E} [(X_i - \\bar{X}_b)^2] julia ac = compute_autocorrelation(rs); julia plot_autocorrelation(ac) \u03b8 autocorrelation \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 1 \u2502\u28a7\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2808\u28e7\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2818\u28e7\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2819\u2877\u2844\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2818\u288e\u2822\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2811\u28cd\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28dd\u2826\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 R \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2811\u282e\u28d7\u28e4\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2809\u281b\u2897\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2811\u2897\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28e7\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u2866\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u281b\u2897\u2866\u28c4\u28c0\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2809\u2819\u281b\u2833\u2836\u28a6\u28e4\u28c4\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 0 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2809\u281b\u28b7\u28e6\u28c0\u28c0\u28c0\u28c0\u28c0\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 0 2000 lag Note that the latter formulation (for multiple chains) is likely to give a more accurate indication of algorithm performance. The code is virtually identical. Just replace rs with the results of a call to run_gelman_diagnostic . Convergence diagnostics The goal of an MCMC analysis is to construct a Markov chain that has the target distribution as its equilibrium distribution, i.e. it has converged. However assessing whether this is the case can be challenging since we do not know the target distribution. Visual inspection of the Markov chain may not be sufficient to diagnose convergence, particularly where the target distribution has local optima. Automated convergence diagnostics are therefore integrated closely with the MCMC functionality in Discuit; the Geweke test for single chains and the Gelman-Rubin for multiple chains. Just like autocorrelation, the latter, provided that the chains have been initialised with over dispersed parameters (with respect to the target distribution), provides a more reliable indication of algorithm performance (in this case, convergence). Geweke test of stationarity The Geweke statistic tests for non-stationarity by comparing the mean and variance for two sections of the Markov chain (see Geweke, 1992; Cowles, 1996). It is given by: z = \\frac{\\bar{\\theta}_{i, \\alpha} - \\bar{\\theta}_{i, \\beta}}{\\sqrt{Var(\\theta_{i, \\alpha})+Var(\\theta_{i, \\beta})})} Geweke statistics are computed automatically for analyses run in Discuit and can be accessed directly (i.e. rs.geweke ) or else inspected using one of the built in tools, e.g: julia plot_geweke_series(rs) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 2 \u2502\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2806\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 z \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2809\u2809\u2809\u2809\u2849\u2809\u2809\u2809\u284d\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u280b\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u284d\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2502 \u2502\u2800\u2800\u2800\u2800\u2801\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2804\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2801\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2804\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2804\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2818\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 -1 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 0 20000 Note that the results of MCMC analyses, including Geweke statistics, can be saved to file for analysis in the companion R package . In Julia, run: print_mcmc_results(rs, path/to/mcmc/data/ ) Now, in R, run: library(Discuit) library(gridExtra) rs = LoadMcmcResults( path/to/mcmc/data/ ) tgt = c(0.003, 0.1) grid.arrange(PlotGewekeSeries(rs), PlotParameterHeatmap(rs, 1, 2, tgt), nrow = 1) Gelman-Rubin convergence diagnostic The Gelman-Rubin diagnostic is designed to diagnose convergence of two or more Markov chains by comparing within chain variance to between chain variance (Gelman et al, 1992, 2014). The estimated scale reduction statistic (sometimes referred to as potential scale reduction factor ) is calculated for each parameter in the model. Let $\\bar{\\theta}$, $W$ and $B$ be vectors of length $P$ representing the mean of model parameters $\\theta$, within chain variance between chain variance respectively for $M$ Markov chains: W = \\frac{1}{M} \\sum_{i = 1}^M \\sigma^2_i B = \\frac{N}{M - 1} \\sum_{i = 1}^M (\\hat{\\theta}_i - \\hat{\\theta})^2 The estimated scale reduction statistic is given by: R = \\sqrt{\\frac{d + 3}{d + 1} \\frac{N-1}{N} + (\\frac{M+1}{MN} \\frac{B}{W})} where the first quantity on the RHS adjusts for sampling variance and $d$ is degrees of freedom estimated using the method of moments. For a valid test of convergence the Gelman-Rubin requires two or more Markov chains with over dispersed target values relative to the target distribution. A matrix of such values is therefore required in place of the vector representing the initial values an McMC analysis when calling the function in Discuit, with the $i^{th}$ row vector used to initialise the $i^{th}$ Markov chain. julia rs = run_gelman_diagnostic(model, y, [0.0025 0.08; 0.003 0.12; 0.0035 0.1]); running gelman diagnostic... (3 chains) chain 1 complete on thread 1 chain 2 complete on thread 1 chain 3 complete on thread 1 finished (sample \u03bc = [0.00305, 0.101]). Simulation The main purpose of the simulation functionality included in Discuit is to provide a source of simulated observations data for evaluation and validation of the MCMC functionality. However simulation can also be an interesting way to explore and better understand the dynamics of the model. To produce the Lotka-Volterra example given in the paper use: julia model = generate_model( LOTKA , [79, 71]); julia xi = gillespie_sim(model, [0.5, 0.0025, 0.3]); running simulation... finished (17520 events). julia plot_trajectory(xi) PN simulation \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 500 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 P \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 N \u2502\u2800\u2800\u2800\u2844\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u28e0\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u28b8\u28a7\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u28ff\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u28b8\u28b8\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u28ff\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2880\u28b8\u28b8\u2840\u2800\u2800\u2800\u2800\u2800\u28b0\u28c7\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u28a0\u2807\u2847\u2800\u2800\u2800\u2800\u28f7\u2800\u2800\u2800\u2502 \u2502\u2800\u28b8\u284f\u2800\u2847\u2800\u2800\u2800\u2800\u2800\u285f\u28b9\u2840\u2800\u2800\u2800\u2800\u2800\u28f6\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u28b8\u2800\u28c7\u2800\u2800\u2800\u28b0\u283f\u2844\u2800\u2800\u2502 population \u2502\u2800\u285e\u28e7\u2800\u28c7\u2800\u2800\u2800\u2800\u28a0\u2807\u2800\u2847\u2800\u2800\u2800\u2800\u28b8\u2819\u2847\u2800\u2800\u28b8\u28b6\u28f4\u2844\u2800\u2800\u28a0\u28fe\u2800\u28b8\u2800\u2800\u2800\u28f8\u2800\u2847\u2800\u2800\u2502 \u2502\u2800\u2847\u28ff\u2800\u28b8\u2800\u2800\u2800\u2800\u28ff\u2840\u2800\u28b3\u2800\u2800\u2800\u2880\u287c\u2800\u28b3\u2800\u2800\u284f\u2808\u2801\u28b3\u2800\u2800\u28b8\u28ff\u2844\u2838\u2844\u2800\u28f0\u28fb\u2800\u28b9\u2800\u2800\u2502 \u2502\u2800\u28c7\u28ff\u2800\u2808\u28e7\u2800\u2800\u28f8\u28fb\u28e7\u2800\u28b8\u2844\u2800\u2800\u28f8\u28a7\u2800\u28b8\u2844\u28a0\u2847\u2800\u2800\u2818\u2847\u2800\u285e\u2847\u2847\u2800\u28c7\u2800\u284f\u284f\u2847\u2808\u2847\u2800\u2502 \u2502\u28b8\u28b9\u28b8\u2800\u2800\u2818\u28c7\u2880\u28ef\u2847\u28b8\u2840\u2800\u28b3\u28e0\u28b6\u281f\u2838\u2844\u2800\u28bf\u28fe\u28b7\u2880\u2800\u2800\u28a7\u2800\u28f7\u2803\u2847\u2800\u28b8\u2880\u28f7\u2803\u2847\u2800\u28b9\u2840\u2502 \u2502\u28b8\u28b8\u2808\u2847\u2800\u2800\u2838\u28fe\u281f\u2801\u2808\u28c7\u2800\u2800\u28bb\u285f\u2800\u2800\u28a7\u2800\u2880\u2847\u2818\u283b\u28e6\u2800\u2818\u28be\u287f\u2800\u28a7\u2800\u2818\u28fe\u287e\u2800\u28a7\u2800\u2800\u28b3\u2502 \u2502\u28cf\u285e\u2800\u28c7\u2800\u2800\u2880\u285f\u2800\u2800\u2800\u28b9\u2840\u28b0\u280b\u2800\u2800\u2800\u2818\u2846\u287e\u2800\u2800\u2800\u2838\u28c6\u28f4\u280f\u2800\u2800\u28b8\u2800\u2880\u287f\u2803\u2800\u2838\u2844\u2880\u28f0\u2502 \u2502\u2808\u2800\u2800\u2839\u2874\u281f\u281e\u2800\u2800\u2800\u2800\u2800\u2833\u280f\u2800\u2800\u2800\u2800\u2800\u281b\u2803\u2800\u2800\u2800\u2800\u2808\u2801\u2800\u2800\u2800\u2808\u28a7\u285f\u2800\u2800\u2800\u2800\u2833\u281e\u2801\u2502 0 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 0 100 time The maximum time of the simulation and number of observations to draw can also be specified, e.g. generate_model(\"LOTKA\", [79, 71], 100.0, 10) . Custom MCMC In addition to the two proposal algorithms included with the packages by default, Discuit allows for users to develop their own algorithms for data augmented MCMC via the alternative custom MCMC framework, while still taking advantage of features including automated, finite adaptive multivariate \u03b8 proposals. In some cases may wish to simply tweak the proposal algorithms included in the source code repositories for minor performance gains in certain models, but the custom MCMC framework can also be useful in cases where the augmented data aspects of the model have specific or complex constraints, such as the next example which is based on an analysis of a smallpox outbreak within a closed community in Abakaliki, Nigeria by O'Neill and Roberts (ADD CITATION). First we generate a standard SIR model and set the t0_index = 3 : julia model = generate_model( SIR , [119, 1, 0]); julia model.t0_index = 3; Next we define the \"medium\" prior used by O'Neill and Roberts, with some help from the Distributions.jl package: Updating registry at `~/.julia/registries/General` Updating git-repo `https://github.com/JuliaRegistries/General.git` \u001b[?25l\u001b[2K\u001b[?25h Resolving package versions... Installed StatsBase \u2500\u2500\u2500\u2500\u2500 v0.30.0 Installed Distributions \u2500 v0.20.0 Updating `~/build/mjb3/Discuit.jl/docs/Project.toml` [31c24e10] + Distributions v0.20.0 Updating `~/build/mjb3/Discuit.jl/docs/Manifest.toml` [31c24e10] \u2191 Distributions v0.19.2 \u21d2 v0.20.0 [2913bbd2] \u2193 StatsBase v0.31.0 \u21d2 v0.30.0 julia using Distributions; julia p1 = Gamma(10, 0.0001); julia p2 = Gamma(10, 0.01); julia function prior_density(parameters::Array{Float64, 1}) return parameters[3] 0.0 ? pdf(p1, parameters[1]) * pdf(p2, parameters[2]) * (0.1 * exp(0.1 * parameters[3])) : 0.0 end prior_density (generic function with 1 method) julia model.prior_density = prior_density; The observation model is replaced with one that returns log(1) since we will only propose sequences consitent with the observed recoveries and $\\pi(\\xi | \\theta)$ is evaluated automatically by Discuit): julia observation_model(y::Array{Int, 1}, population::Array{Int, 1}) = 0.0 observation_model (generic function with 1 method) julia model.observation_model = observation_model; Next we define an array t to contain the recovery times reported by O'Neill and Roberts and a simple Observations variable which consists of the maximum event time and an empty two dimensional array: julia t = [0.0, 13.0, 20.0, 22.0, 25.0, 25.0, 25.0, 26.0, 30.0, 35.0, 38.0, 40.0, 40.0, 42.0, 42.0, 47.0, 50.0, 51.0, 55.0, 55.0, 56.0, 57.0, 58.0, 60.0, 60.0, 61.0, 66.0]; julia y = Observations([67.0], Array{Int64, 2}(undef, 1, 1)) Observations([67.0], [139627917258272]) We also need to define an initial state using the generate_custom_x0 function using some parameter values and a vector of event times and corresponding event types, consistent with t : julia evt_tm = Float64[]; julia evt_tp = Int64[]; julia for i in 1:(length(t) - 1)# infections at arbitrary t t0 push!(evt_tm, -4.0) push!(evt_tp, 1) end julia for i in eachindex(t) # recoveries push!(evt_tm, t[i]) push!(evt_tp, 2) end julia x0 = generate_custom_x0(model, y, [0.001, 0.1, -4.0], evt_tm, evt_tp); The final step before we run our analysis is to define the algorithm which will propose changes to augmented data (parameter proposals are automatically configured by Discuit). Since it is assumed that the total number of events is known we can construct an algorithm that simply changes the time of an event in the trajectory. Events are chosen and new times drawn from uniform distributions ensuring that: g(X_{f \\rightarrow i}) = g(X_{i \\rightarrow f}) such that the terms cancel in the Metropolis-Hastings acceptance equation. Some additional information is available regarding the times of recoveries; they are known to within a day. We therefore propose new recovery event times such that they remain within the time frame of a single time unit, which correspond to days in this model: julia function custom_proposal(model::PrivateDiscuitModel, xi::MarkovState, xf_parameters::ParameterProposal) t0 = xf_parameters.value[model.t0_index] ## move seq_f = deepcopy(xi.trajectory) # choose event and define new one evt_i = rand(1:length(xi.trajectory.time)) evt_tm = xi.trajectory.event_type[evt_i] == 1 ? (rand() * (model.obs_data.time[end] - t0)) + t0 : floor(xi.trajectory.time[evt_i]) + rand() evt_tp = xi.trajectory.event_type[evt_i] # remove old one splice!(seq_f.time, evt_i) splice!(seq_f.event_type, evt_i) # add new one if evt_tm seq_f.time[end] push!(seq_f.time, evt_tm) push!(seq_f.event_type, evt_tp) else for i in eachindex(seq_f.time) if seq_f.time[i] evt_tm insert!(seq_f.time, i, evt_tm) insert!(seq_f.event_type, i, evt_tp) break end end end # compute ln g(x) prop_lk = 1.0 ## evaluate full likelihood for trajectory proposal and return return MarkovState(xi.parameters, seq_f, compute_full_log_like(model, xi.parameters.value, seq_f), prop_lk, 3) end # end of std proposal function custom_proposal (generic function with 1 method) We can now run the MCMC analysis: julia rs = run_custom_mcmc(model, y, custom_proposal, x0, 120000, 20000); running custom MCMC... ERROR: UndefVarError: compute_full_log_like not defined The output from the custom MCMC functionality is in the same format as the those produced using the core functions and thus be analysed in the same way. In this case the results were saved to file and analysed in R, in the manner described above: The traceplots indicate good mixing and the results are fairly similar to that obtained by O'Neill and Roberts given the differences in the models used. References @article{gillespie_exact_1977, title = {Exact stochastic simulation of coupled chemical reactions}, volume = {81}, issn = {0022-3654, 1541-5740}, url = {http://pubs.acs.org/doi/abs/10.1021/j100540a008}, doi = {10.1021/j100540a008}, language = {en}, number = {25}, urldate = {2017-02-18}, journal = {The Journal of Physical Chemistry}, author = {Gillespie, Daniel T.}, month = dec, year = {1977}, pages = {2340--2361} } @incollection{geweke_evaluating_1992, title = {Evaluating the {Accuracy} of {Sampling}-{Based} {Approaches} to the {Calculation} of {Posterior} {Moments}}, abstract = {Data augmentation and Gibbs sampling are two closely related, sampling-based approaches to the calculation of posterior moments. The fact that each produces a sample whose constituents are neither independent nor identically distributed complicates the assessment of convergence and numerical accuracy of the approximations to the expected value of functions of interest under the posterior. In this paper methods from spectral analysis are used to evaluate numerical accuracy formally and construct diagnostics for convergence. These methods are illustrated in the normal linear model with informative priors, and in the Tobit-censored regression model.}, booktitle = {{IN} {BAYESIAN} {STATISTICS}}, publisher = {University Press}, author = {Geweke, John}, year = {1992}, pages = {169--193} } @article{cowles_markov_1996, title = {Markov {Chain} {Monte} {Carlo} {Convergence} {Diagnostics}: {A} {Comparative} {Review}}, volume = {91}, issn = {01621459}, shorttitle = {Markov {Chain} {Monte} {Carlo} {Convergence} {Diagnostics}}, url = {http://www.jstor.org/stable/2291683}, doi = {10.2307/2291683}, number = {434}, urldate = {2018-03-20}, journal = {Journal of the American Statistical Association}, author = {Cowles, Mary Kathryn and Carlin, Bradley P.}, month = jun, year = {1996}, pages = {883} } @article{gelman_inference_1992, title = {Inference from iterative simulation using multiple sequences}, journal = {Statistical science}, author = {Gelman, Andrew and Rubin, Donald B.}, year = {1992}, pages = {457--472} } @book{gelman_bayesian_2014, title = {Bayesian data analysis}, isbn = {978-1-4398-9820-8 978-1-4398-4096-2}, language = {English}, urldate = {2018-03-18}, author = {Gelman, Andrew and Carlin, John B and Stern, Hal Steven and Dunson, David B and Vehtari, Aki and Rubin, Donald B}, year = {2014}, note = {OCLC: 909477393} }","title":"Examples"},{"location":"examples/#discuitjl-examples","text":"The following examples provide a flavour of package's core functionality. See the Discuit.jl manual for a description of the data types and functions in Discuit, and Discuit.jl models for a description of the predefined models available in the package. The tutorial is designed to be run using the REPL but usually it is wise to save code and analyses to a text file for reference later. By convention Julia code files have the extension '.jl'. For example, the file pooley_model.jl (click to download) contains code equivalent to the next section, i.e. it defines a SIS model and stores it in variable model which can then be used just as if we had typed the commands manually. One way to run a code file is to open a command prompt or terminal and cd to the location of the file. Check the code and save the file. Next start the REPL using the command julia . Finally type the name of the file to run the code: ADD GIF","title":"Discuit.jl examples"},{"location":"examples/#defining-a-model","text":"DiscuitModel s can be created automatically using helper functions or manually by specifying each component. For example the model we are about to create could be generated automatically using generate_model(\"SIS\", [100,1]) . However constructing it manually is a helpful exercise for getting to know the package. See Discuit.jl models for further details of the generate_model function. We start by examining DiscuitModel in the package documentation: julia using Discuit; The events in a DiscuitModel are defined by the rates at which they occur and a transition matrix, which governs how individuals migrate between states. In the basic Kermack-McKendrick SIS (and SIR ) model, rates for infection and recovery events respectively are given by: r_1 = \\theta_1 SI r_2 = \\theta_2 I With the transition matrix: T = \\begin{bmatrix} -1 & 1 \\\\ 1 & -1 \\end{bmatrix} Note how the first row of the matrix reflects the removal of a susceptible individual from the S state, and migrates them to the second element of the row, the 'I' state. The code required to represent the rates (or 'rate function') and transition matrxi as Julia variables is correspondingly straightforward: julia function sis_rf(output::Array{Float64, 1}, parameters::Array{Float64, 1}, population::Array{Int64, 1}) output[1] = parameters[1] * population[1] * population[2] output[2] = parameters[2] * population[2] end sis_rf (generic function with 1 method) julia t_matrix = [-1 1; 1 -1] 2\u00d72 Array{Int64,2}: -1 1 1 -1 The output confirms that sis_rf , a generic function with 1 method has been defined and gives a description of the 2 dimensional Array variable that represents the transition matrix (do not copy and paste these bits when running the code on your own machine). Note that the correct function signature must be used in the implementation for it to be compatible with the package. In this case the function takes three Array parameters of a given type, the first of which is the output variable modified by the function (which is why it does not need to return any actual output variable). Next we define a simple observation function, again with the correct signature: julia obs_fn(population::Array{Int64, 1}) = population obs_fn (generic function with 1 method) The default prior distribution is flat and improper, and is equivalent to: julia function weak_prior(parameters::Array{Float64, 1}) parameters[1] 0.0 || return 0.0 parameters[2] 0.0 || return 0.0 return 1.0 end weak_prior (generic function with 1 method) Finally, we define an observation likelihood model. Again, we use the same as Pooley, with observation errors normally distributed around the true value with standard deviation 2 : julia function si_gaussian(y::Array{Int64, 1}, population::Array{Int64, 1}) obs_err = 2 tmp1 = log(1 / (sqrt(2 * pi) * obs_err)) tmp2 = 2 * obs_err * obs_err obs_diff = y[2] - population[2] return tmp1 - ((obs_diff * obs_diff) / tmp2) end si_gaussian (generic function with 1 method) We can now define a model. We must also specify the initial_condition which represents the state of the population at the origin of each trajectory. A final parameter declared inline is an optional index for the t0 parameter (ignore for now): julia initial_condition = [100, 1] 2-element Array{Int64,1}: 100 1 julia model = DiscuitModel( SIS , initial_condition, sis_rf, t_matrix, obs_fn, weak_prior, si_gaussian, 0);","title":"Defining a model"},{"location":"examples/#mcmc","text":"The following example is based on that published by Pooley et al. (2015) in the paper that introduces the model based proposal method. The observations data simulated by Pooley can be downloaded here and saved, e.g. to path/to/data/ . Next, load the observations data using: y = get_observations( path/to/data/pooley.csv ) Now we can run an MCMC analysis based on the simulated datset: julia rs = run_met_hastings_mcmc(model, y, [0.0025, 0.12]); running MCMC... finished (sample \u03bc = [0.0034241420925407253, 0.11487898025205769]) Visual inspection of the Markov chain using the traceplot is one way of assessing the convergence of the algorithm: julia plot_parameter_trace(rs, 1) The marginal distribution of parameters can be plotted by calling: julia plot_parameter_marginal(rs, 1) \u250c Warning: The keyword parameter `bins` is deprecated, use `nbins` instead \u2502 caller = ip:0x0 \u2514 @ Core :-1 \u250c \u2510 [0.001 , 0.0015) \u2524\u2587 295 [0.0015, 0.002 ) \u2524\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587 3378 [0.002 , 0.0025) \u2524\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587 6960 [0.0025, 0.003 ) \u2524\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587 7700 [0.003 , 0.0035) \u2524\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587 7265 [0.0035, 0.004 ) \u2524\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587 5425 [0.004 , 0.0045) \u2524\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587 3148 [0.0045, 0.005 ) \u2524\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587 2092 [0.005 , 0.0055) \u2524\u2587\u2587\u2587\u2587\u2587 1023 [0.0055, 0.006 ) \u2524\u2587\u2587\u2587 675 [0.006 , 0.0065) \u2524\u2587\u2587 494 \u03b8\u2081 [0.0065, 0.007 ) \u2524\u2587 271 [0.007 , 0.0075) \u2524 68 [0.0075, 0.008 ) \u2524\u2587 138 [0.008 , 0.0085) \u2524 76 [0.0085, 0.009 ) \u2524 80 [0.009 , 0.0095) \u2524 111 [0.0095, 0.01 ) \u2524\u2587 273 [0.01 , 0.0105) \u2524\u2587 195 [0.0105, 0.011 ) \u2524\u2587 133 [0.011 , 0.0115) \u2524 67 [0.0115, 0.012 ) \u2524 77 [0.012 , 0.0125) \u2524 57 \u2514 \u2518 samples Plotting the data with third party packages such as PyPlot is simple: using PyPlot x = mcmc.samples[mcmc.adapt_period:size(mcmc.samples, 1), parameter] plt[:hist](x, 30) xlabel(string( \\$\\\\theta_ , parameter, \\\\$ )) ylabel( density ) A pairwise representation can be produced by calling plot_parameter_heatmap (see below for an example).","title":"MCMC"},{"location":"examples/#autocorrelation","text":"Autocorrelation can be used to help determine how well the algorithm mixed by using compute_autocorrelation . The autocorrelation function for a single Markov chain is implemented in Discuit using the standard formula: R_l = \\frac{\\textrm{E} [(X_i - \\bar{X})(X_{i+l} - \\bar{X})]}{\\sigma^2} for any given lag l . The modified formula for multiple chains is given by: R_{b,l} = \\frac{\\textrm{E} [ (X_i - \\bar{X}_b) ( X_{i + l} - \\bar{X}_b ) ]}{\\sigma^2_b} \\sigma^2_b = \\textrm{E} [(X_i - \\bar{X}_b)^2] julia ac = compute_autocorrelation(rs); julia plot_autocorrelation(ac) \u03b8 autocorrelation \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 1 \u2502\u28a7\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2808\u28e7\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2818\u28e7\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2819\u2877\u2844\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2818\u288e\u2822\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2811\u28cd\u28a6\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2833\u28dd\u2826\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 R \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2811\u282e\u28d7\u28e4\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2809\u281b\u2897\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2811\u2897\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2819\u28e7\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u283b\u2866\u28c4\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u281b\u2897\u2866\u28c4\u28c0\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2808\u2809\u2819\u281b\u2833\u2836\u28a6\u28e4\u28c4\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 0 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2809\u281b\u28b7\u28e6\u28c0\u28c0\u28c0\u28c0\u28c0\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 0 2000 lag Note that the latter formulation (for multiple chains) is likely to give a more accurate indication of algorithm performance. The code is virtually identical. Just replace rs with the results of a call to run_gelman_diagnostic .","title":"Autocorrelation"},{"location":"examples/#convergence-diagnostics","text":"The goal of an MCMC analysis is to construct a Markov chain that has the target distribution as its equilibrium distribution, i.e. it has converged. However assessing whether this is the case can be challenging since we do not know the target distribution. Visual inspection of the Markov chain may not be sufficient to diagnose convergence, particularly where the target distribution has local optima. Automated convergence diagnostics are therefore integrated closely with the MCMC functionality in Discuit; the Geweke test for single chains and the Gelman-Rubin for multiple chains. Just like autocorrelation, the latter, provided that the chains have been initialised with over dispersed parameters (with respect to the target distribution), provides a more reliable indication of algorithm performance (in this case, convergence).","title":"Convergence diagnostics"},{"location":"examples/#geweke-test-of-stationarity","text":"The Geweke statistic tests for non-stationarity by comparing the mean and variance for two sections of the Markov chain (see Geweke, 1992; Cowles, 1996). It is given by: z = \\frac{\\bar{\\theta}_{i, \\alpha} - \\bar{\\theta}_{i, \\beta}}{\\sqrt{Var(\\theta_{i, \\alpha})+Var(\\theta_{i, \\beta})})} Geweke statistics are computed automatically for analyses run in Discuit and can be accessed directly (i.e. rs.geweke ) or else inspected using one of the built in tools, e.g: julia plot_geweke_series(rs) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 2 \u2502\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2806\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 z \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2809\u2809\u2809\u2809\u2849\u2809\u2809\u2809\u284d\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u280b\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u284d\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2502 \u2502\u2800\u2800\u2800\u2800\u2801\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2804\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2801\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2804\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2804\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2818\u2502 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 -1 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 0 20000 Note that the results of MCMC analyses, including Geweke statistics, can be saved to file for analysis in the companion R package . In Julia, run: print_mcmc_results(rs, path/to/mcmc/data/ ) Now, in R, run: library(Discuit) library(gridExtra) rs = LoadMcmcResults( path/to/mcmc/data/ ) tgt = c(0.003, 0.1) grid.arrange(PlotGewekeSeries(rs), PlotParameterHeatmap(rs, 1, 2, tgt), nrow = 1)","title":"Geweke test of stationarity"},{"location":"examples/#gelman-rubin-convergence-diagnostic","text":"The Gelman-Rubin diagnostic is designed to diagnose convergence of two or more Markov chains by comparing within chain variance to between chain variance (Gelman et al, 1992, 2014). The estimated scale reduction statistic (sometimes referred to as potential scale reduction factor ) is calculated for each parameter in the model. Let $\\bar{\\theta}$, $W$ and $B$ be vectors of length $P$ representing the mean of model parameters $\\theta$, within chain variance between chain variance respectively for $M$ Markov chains: W = \\frac{1}{M} \\sum_{i = 1}^M \\sigma^2_i B = \\frac{N}{M - 1} \\sum_{i = 1}^M (\\hat{\\theta}_i - \\hat{\\theta})^2 The estimated scale reduction statistic is given by: R = \\sqrt{\\frac{d + 3}{d + 1} \\frac{N-1}{N} + (\\frac{M+1}{MN} \\frac{B}{W})} where the first quantity on the RHS adjusts for sampling variance and $d$ is degrees of freedom estimated using the method of moments. For a valid test of convergence the Gelman-Rubin requires two or more Markov chains with over dispersed target values relative to the target distribution. A matrix of such values is therefore required in place of the vector representing the initial values an McMC analysis when calling the function in Discuit, with the $i^{th}$ row vector used to initialise the $i^{th}$ Markov chain. julia rs = run_gelman_diagnostic(model, y, [0.0025 0.08; 0.003 0.12; 0.0035 0.1]); running gelman diagnostic... (3 chains) chain 1 complete on thread 1 chain 2 complete on thread 1 chain 3 complete on thread 1 finished (sample \u03bc = [0.00305, 0.101]).","title":"Gelman-Rubin convergence diagnostic"},{"location":"examples/#simulation","text":"The main purpose of the simulation functionality included in Discuit is to provide a source of simulated observations data for evaluation and validation of the MCMC functionality. However simulation can also be an interesting way to explore and better understand the dynamics of the model. To produce the Lotka-Volterra example given in the paper use: julia model = generate_model( LOTKA , [79, 71]); julia xi = gillespie_sim(model, [0.5, 0.0025, 0.3]); running simulation... finished (17520 events). julia plot_trajectory(xi) PN simulation \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 500 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 P \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 N \u2502\u2800\u2800\u2800\u2844\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u28e0\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u28b8\u28a7\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u28ff\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2800\u28b8\u28b8\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u28ff\u2840\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2502\u2800\u2880\u28b8\u28b8\u2840\u2800\u2800\u2800\u2800\u2800\u28b0\u28c7\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u28a0\u2807\u2847\u2800\u2800\u2800\u2800\u28f7\u2800\u2800\u2800\u2502 \u2502\u2800\u28b8\u284f\u2800\u2847\u2800\u2800\u2800\u2800\u2800\u285f\u28b9\u2840\u2800\u2800\u2800\u2800\u2800\u28f6\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u28b8\u2800\u28c7\u2800\u2800\u2800\u28b0\u283f\u2844\u2800\u2800\u2502 population \u2502\u2800\u285e\u28e7\u2800\u28c7\u2800\u2800\u2800\u2800\u28a0\u2807\u2800\u2847\u2800\u2800\u2800\u2800\u28b8\u2819\u2847\u2800\u2800\u28b8\u28b6\u28f4\u2844\u2800\u2800\u28a0\u28fe\u2800\u28b8\u2800\u2800\u2800\u28f8\u2800\u2847\u2800\u2800\u2502 \u2502\u2800\u2847\u28ff\u2800\u28b8\u2800\u2800\u2800\u2800\u28ff\u2840\u2800\u28b3\u2800\u2800\u2800\u2880\u287c\u2800\u28b3\u2800\u2800\u284f\u2808\u2801\u28b3\u2800\u2800\u28b8\u28ff\u2844\u2838\u2844\u2800\u28f0\u28fb\u2800\u28b9\u2800\u2800\u2502 \u2502\u2800\u28c7\u28ff\u2800\u2808\u28e7\u2800\u2800\u28f8\u28fb\u28e7\u2800\u28b8\u2844\u2800\u2800\u28f8\u28a7\u2800\u28b8\u2844\u28a0\u2847\u2800\u2800\u2818\u2847\u2800\u285e\u2847\u2847\u2800\u28c7\u2800\u284f\u284f\u2847\u2808\u2847\u2800\u2502 \u2502\u28b8\u28b9\u28b8\u2800\u2800\u2818\u28c7\u2880\u28ef\u2847\u28b8\u2840\u2800\u28b3\u28e0\u28b6\u281f\u2838\u2844\u2800\u28bf\u28fe\u28b7\u2880\u2800\u2800\u28a7\u2800\u28f7\u2803\u2847\u2800\u28b8\u2880\u28f7\u2803\u2847\u2800\u28b9\u2840\u2502 \u2502\u28b8\u28b8\u2808\u2847\u2800\u2800\u2838\u28fe\u281f\u2801\u2808\u28c7\u2800\u2800\u28bb\u285f\u2800\u2800\u28a7\u2800\u2880\u2847\u2818\u283b\u28e6\u2800\u2818\u28be\u287f\u2800\u28a7\u2800\u2818\u28fe\u287e\u2800\u28a7\u2800\u2800\u28b3\u2502 \u2502\u28cf\u285e\u2800\u28c7\u2800\u2800\u2880\u285f\u2800\u2800\u2800\u28b9\u2840\u28b0\u280b\u2800\u2800\u2800\u2818\u2846\u287e\u2800\u2800\u2800\u2838\u28c6\u28f4\u280f\u2800\u2800\u28b8\u2800\u2880\u287f\u2803\u2800\u2838\u2844\u2880\u28f0\u2502 \u2502\u2808\u2800\u2800\u2839\u2874\u281f\u281e\u2800\u2800\u2800\u2800\u2800\u2833\u280f\u2800\u2800\u2800\u2800\u2800\u281b\u2803\u2800\u2800\u2800\u2800\u2808\u2801\u2800\u2800\u2800\u2808\u28a7\u285f\u2800\u2800\u2800\u2800\u2833\u281e\u2801\u2502 0 \u2502\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 0 100 time The maximum time of the simulation and number of observations to draw can also be specified, e.g. generate_model(\"LOTKA\", [79, 71], 100.0, 10) .","title":"Simulation"},{"location":"examples/#custom-mcmc","text":"In addition to the two proposal algorithms included with the packages by default, Discuit allows for users to develop their own algorithms for data augmented MCMC via the alternative custom MCMC framework, while still taking advantage of features including automated, finite adaptive multivariate \u03b8 proposals. In some cases may wish to simply tweak the proposal algorithms included in the source code repositories for minor performance gains in certain models, but the custom MCMC framework can also be useful in cases where the augmented data aspects of the model have specific or complex constraints, such as the next example which is based on an analysis of a smallpox outbreak within a closed community in Abakaliki, Nigeria by O'Neill and Roberts (ADD CITATION). First we generate a standard SIR model and set the t0_index = 3 : julia model = generate_model( SIR , [119, 1, 0]); julia model.t0_index = 3; Next we define the \"medium\" prior used by O'Neill and Roberts, with some help from the Distributions.jl package: Updating registry at `~/.julia/registries/General` Updating git-repo `https://github.com/JuliaRegistries/General.git` \u001b[?25l\u001b[2K\u001b[?25h Resolving package versions... Installed StatsBase \u2500\u2500\u2500\u2500\u2500 v0.30.0 Installed Distributions \u2500 v0.20.0 Updating `~/build/mjb3/Discuit.jl/docs/Project.toml` [31c24e10] + Distributions v0.20.0 Updating `~/build/mjb3/Discuit.jl/docs/Manifest.toml` [31c24e10] \u2191 Distributions v0.19.2 \u21d2 v0.20.0 [2913bbd2] \u2193 StatsBase v0.31.0 \u21d2 v0.30.0 julia using Distributions; julia p1 = Gamma(10, 0.0001); julia p2 = Gamma(10, 0.01); julia function prior_density(parameters::Array{Float64, 1}) return parameters[3] 0.0 ? pdf(p1, parameters[1]) * pdf(p2, parameters[2]) * (0.1 * exp(0.1 * parameters[3])) : 0.0 end prior_density (generic function with 1 method) julia model.prior_density = prior_density; The observation model is replaced with one that returns log(1) since we will only propose sequences consitent with the observed recoveries and $\\pi(\\xi | \\theta)$ is evaluated automatically by Discuit): julia observation_model(y::Array{Int, 1}, population::Array{Int, 1}) = 0.0 observation_model (generic function with 1 method) julia model.observation_model = observation_model; Next we define an array t to contain the recovery times reported by O'Neill and Roberts and a simple Observations variable which consists of the maximum event time and an empty two dimensional array: julia t = [0.0, 13.0, 20.0, 22.0, 25.0, 25.0, 25.0, 26.0, 30.0, 35.0, 38.0, 40.0, 40.0, 42.0, 42.0, 47.0, 50.0, 51.0, 55.0, 55.0, 56.0, 57.0, 58.0, 60.0, 60.0, 61.0, 66.0]; julia y = Observations([67.0], Array{Int64, 2}(undef, 1, 1)) Observations([67.0], [139627917258272]) We also need to define an initial state using the generate_custom_x0 function using some parameter values and a vector of event times and corresponding event types, consistent with t : julia evt_tm = Float64[]; julia evt_tp = Int64[]; julia for i in 1:(length(t) - 1)# infections at arbitrary t t0 push!(evt_tm, -4.0) push!(evt_tp, 1) end julia for i in eachindex(t) # recoveries push!(evt_tm, t[i]) push!(evt_tp, 2) end julia x0 = generate_custom_x0(model, y, [0.001, 0.1, -4.0], evt_tm, evt_tp); The final step before we run our analysis is to define the algorithm which will propose changes to augmented data (parameter proposals are automatically configured by Discuit). Since it is assumed that the total number of events is known we can construct an algorithm that simply changes the time of an event in the trajectory. Events are chosen and new times drawn from uniform distributions ensuring that: g(X_{f \\rightarrow i}) = g(X_{i \\rightarrow f}) such that the terms cancel in the Metropolis-Hastings acceptance equation. Some additional information is available regarding the times of recoveries; they are known to within a day. We therefore propose new recovery event times such that they remain within the time frame of a single time unit, which correspond to days in this model: julia function custom_proposal(model::PrivateDiscuitModel, xi::MarkovState, xf_parameters::ParameterProposal) t0 = xf_parameters.value[model.t0_index] ## move seq_f = deepcopy(xi.trajectory) # choose event and define new one evt_i = rand(1:length(xi.trajectory.time)) evt_tm = xi.trajectory.event_type[evt_i] == 1 ? (rand() * (model.obs_data.time[end] - t0)) + t0 : floor(xi.trajectory.time[evt_i]) + rand() evt_tp = xi.trajectory.event_type[evt_i] # remove old one splice!(seq_f.time, evt_i) splice!(seq_f.event_type, evt_i) # add new one if evt_tm seq_f.time[end] push!(seq_f.time, evt_tm) push!(seq_f.event_type, evt_tp) else for i in eachindex(seq_f.time) if seq_f.time[i] evt_tm insert!(seq_f.time, i, evt_tm) insert!(seq_f.event_type, i, evt_tp) break end end end # compute ln g(x) prop_lk = 1.0 ## evaluate full likelihood for trajectory proposal and return return MarkovState(xi.parameters, seq_f, compute_full_log_like(model, xi.parameters.value, seq_f), prop_lk, 3) end # end of std proposal function custom_proposal (generic function with 1 method) We can now run the MCMC analysis: julia rs = run_custom_mcmc(model, y, custom_proposal, x0, 120000, 20000); running custom MCMC... ERROR: UndefVarError: compute_full_log_like not defined The output from the custom MCMC functionality is in the same format as the those produced using the core functions and thus be analysed in the same way. In this case the results were saved to file and analysed in R, in the manner described above: The traceplots indicate good mixing and the results are fairly similar to that obtained by O'Neill and Roberts given the differences in the models used.","title":"Custom MCMC"},{"location":"examples/#references","text":"@article{gillespie_exact_1977, title = {Exact stochastic simulation of coupled chemical reactions}, volume = {81}, issn = {0022-3654, 1541-5740}, url = {http://pubs.acs.org/doi/abs/10.1021/j100540a008}, doi = {10.1021/j100540a008}, language = {en}, number = {25}, urldate = {2017-02-18}, journal = {The Journal of Physical Chemistry}, author = {Gillespie, Daniel T.}, month = dec, year = {1977}, pages = {2340--2361} } @incollection{geweke_evaluating_1992, title = {Evaluating the {Accuracy} of {Sampling}-{Based} {Approaches} to the {Calculation} of {Posterior} {Moments}}, abstract = {Data augmentation and Gibbs sampling are two closely related, sampling-based approaches to the calculation of posterior moments. The fact that each produces a sample whose constituents are neither independent nor identically distributed complicates the assessment of convergence and numerical accuracy of the approximations to the expected value of functions of interest under the posterior. In this paper methods from spectral analysis are used to evaluate numerical accuracy formally and construct diagnostics for convergence. These methods are illustrated in the normal linear model with informative priors, and in the Tobit-censored regression model.}, booktitle = {{IN} {BAYESIAN} {STATISTICS}}, publisher = {University Press}, author = {Geweke, John}, year = {1992}, pages = {169--193} } @article{cowles_markov_1996, title = {Markov {Chain} {Monte} {Carlo} {Convergence} {Diagnostics}: {A} {Comparative} {Review}}, volume = {91}, issn = {01621459}, shorttitle = {Markov {Chain} {Monte} {Carlo} {Convergence} {Diagnostics}}, url = {http://www.jstor.org/stable/2291683}, doi = {10.2307/2291683}, number = {434}, urldate = {2018-03-20}, journal = {Journal of the American Statistical Association}, author = {Cowles, Mary Kathryn and Carlin, Bradley P.}, month = jun, year = {1996}, pages = {883} } @article{gelman_inference_1992, title = {Inference from iterative simulation using multiple sequences}, journal = {Statistical science}, author = {Gelman, Andrew and Rubin, Donald B.}, year = {1992}, pages = {457--472} } @book{gelman_bayesian_2014, title = {Bayesian data analysis}, isbn = {978-1-4398-9820-8 978-1-4398-4096-2}, language = {English}, urldate = {2018-03-18}, author = {Gelman, Andrew and Carlin, John B and Stern, Hal Steven and Dunson, David B and Vehtari, Aki and Rubin, Donald B}, year = {2014}, note = {OCLC: 909477393} }","title":"References"},{"location":"manual/","text":"Discuit.jl manual See Discuit.jl examples for a brief introduction to the package's core functionality. Discuit.jl manual Types Functions core functionality model helpers utilities visualisation custom MCMC Index References Types # Discuit.DiscuitModel Type . DiscuitModel Fields model_name \u2013 string, e,g, \"SIR\" . initial_condition \u2013 initial condition rate_function \u2013 event rate function. m_transition \u2013 transition matrix. `observation_function \u2013 observation function, use this to add 'noise' to simulated observations. prior_density \u2013 prior density function. observation_model \u2013 observation model likelihood function. t0_index \u2013 index of the parameter that represents the initial time. 0 if fixed at 0.0 . A mutable struct which represents a DSSCT model (see Discuit.jl models for further details). source # Discuit.SimResults Type . SimResults Fields trajectory \u2013 a variable of type Trajectory . population \u2013 gives the state of the system after each event. observations \u2013 variable of type Observations . The results of a simulation. source # Discuit.Trajectory Type . Trajectory Fields time \u2013 event times. event_type \u2013 event type, index of model.rate_function . A single realisation of the model. source # Discuit.Observations Type . Observations Fields time \u2013 observation times. val \u2013 observation values. Examples # pooley dataset pooley = Observations([20, 40, 60, 80, 100], [0 18; 0 65; 0 70; 0 66; 0 67]) Stores one column vector of observation times and one or more column vectors of observation integer values. source # Discuit.MCMCResults Type . MCMCResults Fields samples \u2013 two dimensional array of samples. mc_accepted \u2013 proposal accepted mean \u2013 sample mean. covar \u2013 parameter covariance matrix. proposal_alg \u2013 proposal algorithm. num_obs \u2013 number of observations. adapt_period \u2013 adaptation (i.e. 'burn in') period. geweke \u2013 Geweke convergence diagnostic results ( Tuple of Array s). The results of an MCMC analysis including samples; mean; covariance matrix; adaptation period; and results of the Geweke test of stationarity. source # Discuit.GelmanResults Type . GelmanResults Fields mu \u2013 between chain sample mean. sdw \u2013 within chain standard deviation. sre \u2013 scale reduction factor estimate. sre_ll \u2013 scale reduction factor lower confidence interval. sre_ul \u2013 scale reduction factor upper confidence interval. mcmc \u2013 array of MCMCResults Results of a Gelman Rubin convergence diagnostic including n MCMCResults variables; mu ; and the scale reduction factor estimates ( sre ). source # Discuit.AutocorrelationResults Type . AutocorrelationResults Fields lag \u2013 autocorrelation lag. autocorrelation \u2013 autocorrelation statistics. Results of a call to compute_autocorrelation . source Functions This section is organised in three parts: the main package core functionality for working with standard Discuit models utilities , for loading to and from file custom MCMC , for running custom algorithms core functionality # Discuit.set_random_seed Function . set_random_seed(seed) Examples set_random_seed(1234) Does what it says on the tin but only if you give it an integer. source # Discuit.gillespie_sim Function . gillespie_sim(model, parameters, tmax = 100.0, num_obs = 5) Parameters model \u2013 DiscuitModel (see [Discuit.jl models]@ref). parameters \u2013 model parameters. tmax \u2013 maximum time. num_obs \u2013 number of observations to draw, Run a DGA simulation on model . Returns a SimResults containing the trajectory and observations data. source # Discuit.run_met_hastings_mcmc Function . run_met_hastings_mcmc(model, obs_data, initial_parameters, steps = 50000, adapt_period = 10000, mbp = true, ppp = 0.3) Parameters model \u2013 DiscuitModel (see [Discuit.jl models]@ref). obs_data \u2013 Observations data. initial_parameters \u2013 initial model parameters (i.e. sample). steps \u2013 number of iterations. mbp \u2013 model based proposals (MBP). Set mbp = false for standard proposals. ppp \u2013 the proportion of parameter (vs. trajectory) proposals. Default: 30%. NB. not required for MBP. Run an MCMC analysis based on model and obs_data of type Observations . The number of samples obtained is equal to steps - adapt_period . source # Discuit.run_gelman_diagnostic Function . run_gelman_diagnostic(model, obs_data, initial_parameters, steps = 50000, adapt_period = 10000, mbp = true, ppp = 0.3) Parameters model \u2013 DiscuitModel (see [Discuit.jl models]@ref). obs_data \u2013 Observations data. initial_parameters \u2013 matrix of initial model parameters. Each column vector correspondes to a single model parameter. steps \u2013 number of iterations. adapt_period \u2013 number of discarded samples. mbp \u2013 model based proposals (MBP). Set mbp = false for standard proposals. ppp \u2013 the proportion of parameter (vs. trajectory) proposals. Default: 30%. NB. not required for MBP. Run n (equal to the number of rows in initial_parameters ) MCMC analyses and perform a Gelman-Rubin convergence diagnostic on the results. NEED TO OVERLOAD AND EXPAND. source # Discuit.compute_autocorrelation Function . compute_autocorrelation(mcmc, lags = 200) Parameters mcmc \u2013 MCMCResults variable. lags \u2013 the number of lags to compute. Default: 200. Compute autocorrelation R for a single Markov chain. Autocorrelation can be used to help determine how well the algorithm mixed by using compute_autocorrelation(rs.mcmc) . The autocorrelation function for a single Markov chain is implemented in Discuit using the standard formula: R_l = \\frac{\\textrm{E} [(X_i - \\bar{X})(X_{i+l} - \\bar{X})]}{\\sigma^2} for any given lag l up to lags (default: 200). source compute_autocorrelation(mcmc, lags = 200) Parameters mcmc \u2013 an array of MCMCResults variables. lags \u2013 the number of lags to compute. Default: 200. Compute autocorrelation R' for a two or more Markov chains. The formula for multiple chains is given by: R^{\\prime}_l = \\frac{\\textrm{E} [ (X_i - \\bar{X}_b) ( X_{i + l} - \\bar{X}_b ) ]}{\\sigma^2_b} \\sigma^2_b = \\textrm{E} [(X_i - \\bar{X}_b)^2] for any given lag l up to lags (default: 200). source model helpers Discuit.jl includes tools for generating components which can help minimise the amount of work required to generate customised DiscuitModel s, including generate_model(...) which is used to access a library of pre defined Discuit.jl models . # Discuit.generate_generic_obs_function Function . generate_generic_obs_function() Generates a simple observation function for use in a DiscuitModel . Not very realistic... source # Discuit.generate_weak_prior Method . generate_weak_prior(n) Parameters n \u2013 the number of parameters in the model. Examples generate_weak_prior(1) Generate a \"weak\" prior density function, where n is the number of parameters in the model. source # Discuit.generate_gaussian_obs_model Function . generate_gaussian_obs_model(n, \u03c3 = 2.0) Parameters n \u2013 the number of discrete states in the model. \u03c3 \u2013 observation error. test latex eqn: rac{n!}{k!(n - k)!} = \binom{n}{k} Examples p = generate_weak_prior(1) Generate a Gaussian observation model for a model with n states. Optionally specify observation error \u03c3 . source # Discuit.generate_model Function . generate_model(model_name, initial_condition, \u03c3 = 2.0) Parameters model_name \u2013 the model, e.g. \"SI\"; \"SIR\"; \"SEIR\"; etc initial_condition \u2013 initial condition. \u03c3 \u2013 observation error. model_name options \"SI\" \"SIR\" \"SIS\" \"SEI\" \"SEIR\" \"SEIS\" \"SEIRS\" \"PREDPREY\" \"ROSSMAC\" Examples generate_model( SIS , [100,1]) Generates a DiscuitModel . Optionally specify observation error \u03c3 . source utilities # Discuit.print_trajectory Function . print_trajectory(model, sim_results, fpath) Parameters model \u2013 DiscuitModel (see [Discuit.jl models]@ref). sim_results \u2013 SimResults variable. fpath \u2013 the destination file path. Save an augmented trajectory from a variable of type SimResults (i.e. from a call to gillespie_sim , see Simulation ) to the file fpath , e.g. \"./out/sim.csv\". source # Discuit.print_observations Function . print_observations(obs_data, fpath) Parameters obs_data \u2013 Observations data. fpath \u2013 the destination file path. Save a set of observations (e.g. from a SimResults obtained by a call to gillespie_sim to the file fpath , e.g. \"./out/obs.csv\". source # Discuit.get_observations Function . get_observations(source) Parameters source \u2013 Array , DataFrame or filepath (i.e. String ) containing the data (with times in the first column). Create and return a variable of type Observations based on a two dimensional array, DataFrame or file location. source # Discuit.tabulate_mcmc_results Function . tabulate_mcmc_results Parameters results \u2013 MCMCResults object. proposals \u2013 display proposal analysis. Display the results of an MCMC analysis. source # Discuit.print_mcmc_results Function . print_mcmc_results(mcmc, dpath) Parameters results \u2013 MCMCResults variable. dpath \u2013 the path of the directory where the results will be saved. Save the results from a call to run_met_hastings_mcmc or run_custom_mcmc to the directory dpath , e.g. \"./out/mcmc/\". source # Discuit.tabulate_gelman_results Function . tabulate_gelman_results Parameters results \u2013 the results of a call to run_gelman_diagnostic . proposals \u2013 display proposal analysis. Display the results of a multi chain analysis run using run_gelman_diagnostic . source # Discuit.print_gelman_results Function . print_gelman_results(results::GelmanResults, dpath::String) Parameters results \u2013 GelmanResults variable. dpath \u2013 the path of the directory where the results will be saved. Save the results from a call to run_gelman_diagnostic to the directory dpath , e.g. \"./out/gelman/\". source # Discuit.print_autocorrelation Function . print_autocorrelation(autocorrelation, fpath) Parameters autocorrelation \u2013 the results of a call to compute_autocorrelation . fpath \u2013 the file path of the destination file. Save the results from a call to compute_autocorrelation to the file fpath , e.g. \"./out/ac.csv\". source visualisation # Discuit.plot_trajectory Function . plot_trajectory(x) Parameters x \u2013 SimResults , i.e. from a call to gillespie_sim . Plot the trajectory of a a DGA simulation on model using UnicodePlots.jl . source # Discuit.plot_parameter_trace Function . plot_parameter_trace(mcmc, parameter) Parameters mcmc \u2013 MCMCResults , e.g. from a call to run_met_hastings_mcmc . parameter \u2013 the index of the model parameter to be plotted. Trace plot of samples from an MCMC analysis for a given model parameter using UnicodePlots.jl . source plot_parameter_trace(mcmc, parameter) Parameters mcmc \u2013 array of MCMCResults , e.g. from a call to run_gelman_diagnostic . parameter \u2013 the index of the model parameter to be plotted. Trace plot of samples from n MCMC analyses for a given model parameter using UnicodePlots.jl . source # Discuit.plot_parameter_marginal Function . plot_parameter_marginal(mcmc, parameter) Parameters mcmc \u2013 MCMCResults , e.g. from a call to run_met_hastings_mcmc . parameter \u2013 the index of the model parameter to be plotted. Plot the marginal distribution of samples from an MCMC analysis for a given model parameter using UnicodePlots.jl . source # Discuit.plot_parameter_heatmap Function . plot_parameter_heatmap(mcmc, x_parameter, y_parameter) Parameters mcmc \u2013 MCMCResults , e.g. from a call to run_met_hastings_mcmc . x_parameter \u2013 the index of the model parameter to be plotted on the x axis. y_parameter \u2013 the index of the model parameter to be plotted on the y axis. Plot the marginal distribution of samples from an MCMC analysis for two model parameters using UnicodePlots.jl . source # Discuit.plot_geweke_series Function . plot_geweke_series(mcmc) Parameters mcmc \u2013 MCMCResults , e.g. from a call to run_met_hastings_mcmc . Plot the Geweke series... source # Discuit.plot_autocorrelation Function . plot_autocorrelation(autocorrelation) Parameters autocorrelation \u2013 The results from a call to compute_autocorrelation . Plot autocorrelation for an MCMC analysis. source custom MCMC # Discuit.run_custom_mcmc Function . run_custom_mcmc(model, obs_data, proposal_function, x0, steps = 50000, adapt_period = 10000, prop_param = false, ppp = 0.3) Parameters model \u2013 DiscuitModel (see [Discuit.jl models]@ref). obs_data \u2013 Observations data. proposal_function \u2013 Function for proposing changes to the trajectory. Must have the signature: custom_proposal(model::PrivateDiscuitModel, xi::MarkovState, xf_parameters::ParameterProposal) = MarkovState(...) x0 \u2013 MarkovState representing the initial sample and trajectory. steps \u2013 number of iterations. adapt_period \u2013 burn in period. prop_param \u2013 simulaneously propose changes to parameters. Default: false . ppp \u2013 the proportion of parameter (vs. trajectory) proposals. Default: 30%. NB. not relevant if prop_param = true . Run a custom MCMC analysis. Similar to run_met_hastings_mcmc except that the proposal_function (of type Function) and initial state x0 (of type MarkovState) are user defined. source # Discuit.run_custom_mcmc_gelman_diagnostic Function . run_custom_mcmc_gelman_diagnostic(m_model, obs_data, proposal_function, x0, initial_parameters, steps = 50000, adapt_period = 10000, mbp = true, ppp = 0.3) Parameters model \u2013 DiscuitModel (see [Discuit.jl models]@ref). obs_data \u2013 Observations data. proposal_function \u2013 Function for proposing changes to the trajectory. Must have the signature: custom_proposal(model::PrivateDiscuitModel, xi::MarkovState, xf_parameters::ParameterProposal) = MarkovState(...) x0 \u2013 vector of MarkovState s representing the initial samples. initial_parameters \u2013 matrix of initial model parameters. Each column vector correspondes to a single model parameter. steps \u2013 number of iterations. adapt_period \u2013 number of discarded samples. prop_param \u2013 simulaneously propose changes to parameters. Default: false . ppp \u2013 the proportion of parameter (vs. trajectory) proposals. Default: 30%. NB. not required for MBP. Run n (equal to the number of rows in initial_parameters ) custom MCMC analyses and perform a Gelman-Rubin convergence diagnostic on the results. NEED TO OVERLOAD AND EXPAND. source # Discuit.generate_custom_x0 Function . generate_custom_x0(model, obs_data, parameters, event_times, event_types) Parameters model \u2013 DiscuitModel (see [Discuit.jl models]@ref). obs_data \u2013 Observations data. parameters \u2013 Array of initial model parameters. event_times \u2013 Array of floats representing the event times. event_types \u2013 Array of integer event types. Generate an initial MarkovState for use in a custom MCMC algorithm. source Index Discuit Discuit.AutocorrelationResults Discuit.DiscuitModel Discuit.GelmanResults Discuit.MCMCResults Discuit.Observations Discuit.SimResults Discuit.Trajectory Discuit.compute_autocorrelation Discuit.generate_custom_x0 Discuit.generate_gaussian_obs_model Discuit.generate_generic_obs_function Discuit.generate_model Discuit.generate_weak_prior Discuit.get_observations Discuit.gillespie_sim Discuit.plot_autocorrelation Discuit.plot_geweke_series Discuit.plot_parameter_heatmap Discuit.plot_parameter_marginal Discuit.plot_parameter_trace Discuit.plot_trajectory Discuit.print_autocorrelation Discuit.print_gelman_results Discuit.print_mcmc_results Discuit.print_observations Discuit.print_trajectory Discuit.run_custom_mcmc Discuit.run_custom_mcmc_gelman_diagnostic Discuit.run_gelman_diagnostic Discuit.run_met_hastings_mcmc Discuit.set_random_seed Discuit.tabulate_gelman_results Discuit.tabulate_mcmc_results References TBA","title":"Manual"},{"location":"manual/#discuitjl-manual","text":"See Discuit.jl examples for a brief introduction to the package's core functionality. Discuit.jl manual Types Functions core functionality model helpers utilities visualisation custom MCMC Index References","title":"Discuit.jl manual"},{"location":"manual/#types","text":"# Discuit.DiscuitModel Type . DiscuitModel Fields model_name \u2013 string, e,g, \"SIR\" . initial_condition \u2013 initial condition rate_function \u2013 event rate function. m_transition \u2013 transition matrix. `observation_function \u2013 observation function, use this to add 'noise' to simulated observations. prior_density \u2013 prior density function. observation_model \u2013 observation model likelihood function. t0_index \u2013 index of the parameter that represents the initial time. 0 if fixed at 0.0 . A mutable struct which represents a DSSCT model (see Discuit.jl models for further details). source # Discuit.SimResults Type . SimResults Fields trajectory \u2013 a variable of type Trajectory . population \u2013 gives the state of the system after each event. observations \u2013 variable of type Observations . The results of a simulation. source # Discuit.Trajectory Type . Trajectory Fields time \u2013 event times. event_type \u2013 event type, index of model.rate_function . A single realisation of the model. source # Discuit.Observations Type . Observations Fields time \u2013 observation times. val \u2013 observation values. Examples # pooley dataset pooley = Observations([20, 40, 60, 80, 100], [0 18; 0 65; 0 70; 0 66; 0 67]) Stores one column vector of observation times and one or more column vectors of observation integer values. source # Discuit.MCMCResults Type . MCMCResults Fields samples \u2013 two dimensional array of samples. mc_accepted \u2013 proposal accepted mean \u2013 sample mean. covar \u2013 parameter covariance matrix. proposal_alg \u2013 proposal algorithm. num_obs \u2013 number of observations. adapt_period \u2013 adaptation (i.e. 'burn in') period. geweke \u2013 Geweke convergence diagnostic results ( Tuple of Array s). The results of an MCMC analysis including samples; mean; covariance matrix; adaptation period; and results of the Geweke test of stationarity. source # Discuit.GelmanResults Type . GelmanResults Fields mu \u2013 between chain sample mean. sdw \u2013 within chain standard deviation. sre \u2013 scale reduction factor estimate. sre_ll \u2013 scale reduction factor lower confidence interval. sre_ul \u2013 scale reduction factor upper confidence interval. mcmc \u2013 array of MCMCResults Results of a Gelman Rubin convergence diagnostic including n MCMCResults variables; mu ; and the scale reduction factor estimates ( sre ). source # Discuit.AutocorrelationResults Type . AutocorrelationResults Fields lag \u2013 autocorrelation lag. autocorrelation \u2013 autocorrelation statistics. Results of a call to compute_autocorrelation . source","title":"Types"},{"location":"manual/#functions","text":"This section is organised in three parts: the main package core functionality for working with standard Discuit models utilities , for loading to and from file custom MCMC , for running custom algorithms","title":"Functions"},{"location":"manual/#core-functionality","text":"# Discuit.set_random_seed Function . set_random_seed(seed) Examples set_random_seed(1234) Does what it says on the tin but only if you give it an integer. source # Discuit.gillespie_sim Function . gillespie_sim(model, parameters, tmax = 100.0, num_obs = 5) Parameters model \u2013 DiscuitModel (see [Discuit.jl models]@ref). parameters \u2013 model parameters. tmax \u2013 maximum time. num_obs \u2013 number of observations to draw, Run a DGA simulation on model . Returns a SimResults containing the trajectory and observations data. source # Discuit.run_met_hastings_mcmc Function . run_met_hastings_mcmc(model, obs_data, initial_parameters, steps = 50000, adapt_period = 10000, mbp = true, ppp = 0.3) Parameters model \u2013 DiscuitModel (see [Discuit.jl models]@ref). obs_data \u2013 Observations data. initial_parameters \u2013 initial model parameters (i.e. sample). steps \u2013 number of iterations. mbp \u2013 model based proposals (MBP). Set mbp = false for standard proposals. ppp \u2013 the proportion of parameter (vs. trajectory) proposals. Default: 30%. NB. not required for MBP. Run an MCMC analysis based on model and obs_data of type Observations . The number of samples obtained is equal to steps - adapt_period . source # Discuit.run_gelman_diagnostic Function . run_gelman_diagnostic(model, obs_data, initial_parameters, steps = 50000, adapt_period = 10000, mbp = true, ppp = 0.3) Parameters model \u2013 DiscuitModel (see [Discuit.jl models]@ref). obs_data \u2013 Observations data. initial_parameters \u2013 matrix of initial model parameters. Each column vector correspondes to a single model parameter. steps \u2013 number of iterations. adapt_period \u2013 number of discarded samples. mbp \u2013 model based proposals (MBP). Set mbp = false for standard proposals. ppp \u2013 the proportion of parameter (vs. trajectory) proposals. Default: 30%. NB. not required for MBP. Run n (equal to the number of rows in initial_parameters ) MCMC analyses and perform a Gelman-Rubin convergence diagnostic on the results. NEED TO OVERLOAD AND EXPAND. source # Discuit.compute_autocorrelation Function . compute_autocorrelation(mcmc, lags = 200) Parameters mcmc \u2013 MCMCResults variable. lags \u2013 the number of lags to compute. Default: 200. Compute autocorrelation R for a single Markov chain. Autocorrelation can be used to help determine how well the algorithm mixed by using compute_autocorrelation(rs.mcmc) . The autocorrelation function for a single Markov chain is implemented in Discuit using the standard formula: R_l = \\frac{\\textrm{E} [(X_i - \\bar{X})(X_{i+l} - \\bar{X})]}{\\sigma^2} for any given lag l up to lags (default: 200). source compute_autocorrelation(mcmc, lags = 200) Parameters mcmc \u2013 an array of MCMCResults variables. lags \u2013 the number of lags to compute. Default: 200. Compute autocorrelation R' for a two or more Markov chains. The formula for multiple chains is given by: R^{\\prime}_l = \\frac{\\textrm{E} [ (X_i - \\bar{X}_b) ( X_{i + l} - \\bar{X}_b ) ]}{\\sigma^2_b} \\sigma^2_b = \\textrm{E} [(X_i - \\bar{X}_b)^2] for any given lag l up to lags (default: 200). source","title":"core functionality"},{"location":"manual/#model-helpers","text":"Discuit.jl includes tools for generating components which can help minimise the amount of work required to generate customised DiscuitModel s, including generate_model(...) which is used to access a library of pre defined Discuit.jl models . # Discuit.generate_generic_obs_function Function . generate_generic_obs_function() Generates a simple observation function for use in a DiscuitModel . Not very realistic... source # Discuit.generate_weak_prior Method . generate_weak_prior(n) Parameters n \u2013 the number of parameters in the model. Examples generate_weak_prior(1) Generate a \"weak\" prior density function, where n is the number of parameters in the model. source # Discuit.generate_gaussian_obs_model Function . generate_gaussian_obs_model(n, \u03c3 = 2.0) Parameters n \u2013 the number of discrete states in the model. \u03c3 \u2013 observation error. test latex eqn: rac{n!}{k!(n - k)!} = \binom{n}{k} Examples p = generate_weak_prior(1) Generate a Gaussian observation model for a model with n states. Optionally specify observation error \u03c3 . source # Discuit.generate_model Function . generate_model(model_name, initial_condition, \u03c3 = 2.0) Parameters model_name \u2013 the model, e.g. \"SI\"; \"SIR\"; \"SEIR\"; etc initial_condition \u2013 initial condition. \u03c3 \u2013 observation error. model_name options \"SI\" \"SIR\" \"SIS\" \"SEI\" \"SEIR\" \"SEIS\" \"SEIRS\" \"PREDPREY\" \"ROSSMAC\" Examples generate_model( SIS , [100,1]) Generates a DiscuitModel . Optionally specify observation error \u03c3 . source","title":"model helpers"},{"location":"manual/#utilities","text":"# Discuit.print_trajectory Function . print_trajectory(model, sim_results, fpath) Parameters model \u2013 DiscuitModel (see [Discuit.jl models]@ref). sim_results \u2013 SimResults variable. fpath \u2013 the destination file path. Save an augmented trajectory from a variable of type SimResults (i.e. from a call to gillespie_sim , see Simulation ) to the file fpath , e.g. \"./out/sim.csv\". source # Discuit.print_observations Function . print_observations(obs_data, fpath) Parameters obs_data \u2013 Observations data. fpath \u2013 the destination file path. Save a set of observations (e.g. from a SimResults obtained by a call to gillespie_sim to the file fpath , e.g. \"./out/obs.csv\". source # Discuit.get_observations Function . get_observations(source) Parameters source \u2013 Array , DataFrame or filepath (i.e. String ) containing the data (with times in the first column). Create and return a variable of type Observations based on a two dimensional array, DataFrame or file location. source # Discuit.tabulate_mcmc_results Function . tabulate_mcmc_results Parameters results \u2013 MCMCResults object. proposals \u2013 display proposal analysis. Display the results of an MCMC analysis. source # Discuit.print_mcmc_results Function . print_mcmc_results(mcmc, dpath) Parameters results \u2013 MCMCResults variable. dpath \u2013 the path of the directory where the results will be saved. Save the results from a call to run_met_hastings_mcmc or run_custom_mcmc to the directory dpath , e.g. \"./out/mcmc/\". source # Discuit.tabulate_gelman_results Function . tabulate_gelman_results Parameters results \u2013 the results of a call to run_gelman_diagnostic . proposals \u2013 display proposal analysis. Display the results of a multi chain analysis run using run_gelman_diagnostic . source # Discuit.print_gelman_results Function . print_gelman_results(results::GelmanResults, dpath::String) Parameters results \u2013 GelmanResults variable. dpath \u2013 the path of the directory where the results will be saved. Save the results from a call to run_gelman_diagnostic to the directory dpath , e.g. \"./out/gelman/\". source # Discuit.print_autocorrelation Function . print_autocorrelation(autocorrelation, fpath) Parameters autocorrelation \u2013 the results of a call to compute_autocorrelation . fpath \u2013 the file path of the destination file. Save the results from a call to compute_autocorrelation to the file fpath , e.g. \"./out/ac.csv\". source","title":"utilities"},{"location":"manual/#visualisation","text":"# Discuit.plot_trajectory Function . plot_trajectory(x) Parameters x \u2013 SimResults , i.e. from a call to gillespie_sim . Plot the trajectory of a a DGA simulation on model using UnicodePlots.jl . source # Discuit.plot_parameter_trace Function . plot_parameter_trace(mcmc, parameter) Parameters mcmc \u2013 MCMCResults , e.g. from a call to run_met_hastings_mcmc . parameter \u2013 the index of the model parameter to be plotted. Trace plot of samples from an MCMC analysis for a given model parameter using UnicodePlots.jl . source plot_parameter_trace(mcmc, parameter) Parameters mcmc \u2013 array of MCMCResults , e.g. from a call to run_gelman_diagnostic . parameter \u2013 the index of the model parameter to be plotted. Trace plot of samples from n MCMC analyses for a given model parameter using UnicodePlots.jl . source # Discuit.plot_parameter_marginal Function . plot_parameter_marginal(mcmc, parameter) Parameters mcmc \u2013 MCMCResults , e.g. from a call to run_met_hastings_mcmc . parameter \u2013 the index of the model parameter to be plotted. Plot the marginal distribution of samples from an MCMC analysis for a given model parameter using UnicodePlots.jl . source # Discuit.plot_parameter_heatmap Function . plot_parameter_heatmap(mcmc, x_parameter, y_parameter) Parameters mcmc \u2013 MCMCResults , e.g. from a call to run_met_hastings_mcmc . x_parameter \u2013 the index of the model parameter to be plotted on the x axis. y_parameter \u2013 the index of the model parameter to be plotted on the y axis. Plot the marginal distribution of samples from an MCMC analysis for two model parameters using UnicodePlots.jl . source # Discuit.plot_geweke_series Function . plot_geweke_series(mcmc) Parameters mcmc \u2013 MCMCResults , e.g. from a call to run_met_hastings_mcmc . Plot the Geweke series... source # Discuit.plot_autocorrelation Function . plot_autocorrelation(autocorrelation) Parameters autocorrelation \u2013 The results from a call to compute_autocorrelation . Plot autocorrelation for an MCMC analysis. source","title":"visualisation"},{"location":"manual/#custom-mcmc","text":"# Discuit.run_custom_mcmc Function . run_custom_mcmc(model, obs_data, proposal_function, x0, steps = 50000, adapt_period = 10000, prop_param = false, ppp = 0.3) Parameters model \u2013 DiscuitModel (see [Discuit.jl models]@ref). obs_data \u2013 Observations data. proposal_function \u2013 Function for proposing changes to the trajectory. Must have the signature: custom_proposal(model::PrivateDiscuitModel, xi::MarkovState, xf_parameters::ParameterProposal) = MarkovState(...) x0 \u2013 MarkovState representing the initial sample and trajectory. steps \u2013 number of iterations. adapt_period \u2013 burn in period. prop_param \u2013 simulaneously propose changes to parameters. Default: false . ppp \u2013 the proportion of parameter (vs. trajectory) proposals. Default: 30%. NB. not relevant if prop_param = true . Run a custom MCMC analysis. Similar to run_met_hastings_mcmc except that the proposal_function (of type Function) and initial state x0 (of type MarkovState) are user defined. source # Discuit.run_custom_mcmc_gelman_diagnostic Function . run_custom_mcmc_gelman_diagnostic(m_model, obs_data, proposal_function, x0, initial_parameters, steps = 50000, adapt_period = 10000, mbp = true, ppp = 0.3) Parameters model \u2013 DiscuitModel (see [Discuit.jl models]@ref). obs_data \u2013 Observations data. proposal_function \u2013 Function for proposing changes to the trajectory. Must have the signature: custom_proposal(model::PrivateDiscuitModel, xi::MarkovState, xf_parameters::ParameterProposal) = MarkovState(...) x0 \u2013 vector of MarkovState s representing the initial samples. initial_parameters \u2013 matrix of initial model parameters. Each column vector correspondes to a single model parameter. steps \u2013 number of iterations. adapt_period \u2013 number of discarded samples. prop_param \u2013 simulaneously propose changes to parameters. Default: false . ppp \u2013 the proportion of parameter (vs. trajectory) proposals. Default: 30%. NB. not required for MBP. Run n (equal to the number of rows in initial_parameters ) custom MCMC analyses and perform a Gelman-Rubin convergence diagnostic on the results. NEED TO OVERLOAD AND EXPAND. source # Discuit.generate_custom_x0 Function . generate_custom_x0(model, obs_data, parameters, event_times, event_types) Parameters model \u2013 DiscuitModel (see [Discuit.jl models]@ref). obs_data \u2013 Observations data. parameters \u2013 Array of initial model parameters. event_times \u2013 Array of floats representing the event times. event_types \u2013 Array of integer event types. Generate an initial MarkovState for use in a custom MCMC algorithm. source","title":"custom MCMC"},{"location":"manual/#index","text":"Discuit Discuit.AutocorrelationResults Discuit.DiscuitModel Discuit.GelmanResults Discuit.MCMCResults Discuit.Observations Discuit.SimResults Discuit.Trajectory Discuit.compute_autocorrelation Discuit.generate_custom_x0 Discuit.generate_gaussian_obs_model Discuit.generate_generic_obs_function Discuit.generate_model Discuit.generate_weak_prior Discuit.get_observations Discuit.gillespie_sim Discuit.plot_autocorrelation Discuit.plot_geweke_series Discuit.plot_parameter_heatmap Discuit.plot_parameter_marginal Discuit.plot_parameter_trace Discuit.plot_trajectory Discuit.print_autocorrelation Discuit.print_gelman_results Discuit.print_mcmc_results Discuit.print_observations Discuit.print_trajectory Discuit.run_custom_mcmc Discuit.run_custom_mcmc_gelman_diagnostic Discuit.run_gelman_diagnostic Discuit.run_met_hastings_mcmc Discuit.set_random_seed Discuit.tabulate_gelman_results Discuit.tabulate_mcmc_results","title":"Index"},{"location":"manual/#references","text":"TBA","title":"References"},{"location":"models/","text":"Discuit.jl models This section describes the library of predefined models in Discuit. Models can also be user defined, or generated and modified according to need. The models included are mostly epidemiological (see Miscellaneous for other types of model). The next section described the model generating function and an overview of default model components. Generating models Pre defined models can be invoked by calling: generate_model(model_name, initial_condition, \u03c3 = 2.0) Examples model = generate_model( SIS , [100, 1]]); model = generate_model( SEIR , [100, 0, 1, 0]], 1.0) Defaults Discuit.jl models are mutable struct s so it is convenient to generate pre defined models with default values for most things and overwrite as required. Only the model_name and initial_condition are required to call generate_model . The following gives an overview of important defaults that the user should note before proceeding with their analysis. Observation function The default model.obs_function returns a vector representing the state of the system at obsertion time $t_y$, i.e.: obs_fn(population::Array{Int64, 1}) = population Observation likelihood model The default model.observation_model is Gaussian with observation error \u03c3 = 2.0 by default, which can be changed by providing an optional third parameter to generate_model per the example above, ADD latex. Prior density function The default model.prior density function is weak, e.g. in a simple two parameter model it is equivalent to: function weak_prior(parameters::Array{Float64, 1}) parameters[1] 0.0 || return 0.0 parameters[2] 0.0 || return 0.0 return 1.0 end Note that all models generated by the ADD XREF have t0_index = 0 by default and that changing this parameter will likely require replacing the the density with something like: function weak_prior(parameters::Array{Float64, 1}) parameters[1] 0.0 || return 0.0 parameters[2] 0.0 || return 0.0 parameters[3] 0.0 || return 0.0 return 1.0 end where parameters[3] is designated as the, e.g. initial infection, which is assumed to have taken place prior to e.g. the initial observation at t = 0.0. Standard Kermack-McKendrick models The standard Kermack-McKendrick SIR model can be used to model diseases which confer lasting immunity or situations where infected individuals have been detected and quarantined for treatment (Kermack and McKendrick, 1991). ADD REF: kermack contributions 1991 Discuit.generate_model( SI , [100, 1]) The susceptible-infectious ( \"SI\" ) is a very basic model with only one type of event. Individuals who become infected remain infected for the duration of trajectory: Discuit.generate_model( SIR , [100, 1, 0]) The classic Kermack-McKendrick susceptible-infectious-recovered ( \"SIR\" ) model includes an extra 'recovery' event and an additional compartment for recovered individuals: Discuit.generate_model( SIS , [100, 1]) The susceptible-infectious-susceptible ( \"SIS\" ) model is an extension of the classic Kermack-McKendrick SIR model for diseases which do not confer lasting immunity: Discuit.generate_model( SIRS , [100, 1, 0]) The susceptible-infectious-recovered-susceptible ( \"SIRS\" ) model incorporates all of the above, i.e. it is for diseases which do not confer long lasting immunity. Latent Kermack-McKendrick models The next class of models extend the classic Kermack-McKendrick by accounting for an exposed state E between infection and the onset of infectiousness. For example, the susceptible-exposed-infectious SEI model: Discuit.generate_model( SEI , [100, 1, 0]) The latent Kermack-McKendrick models can be extended in the same way, i.e, the SEIR model, SEIS and SEIRS : Discuit.generate_model( SEIR , [100, 1, 0, 0]) Discuit.generate_model( SEIS , [100, 1, 0]) Discuit.generate_model( SEIRS , [100, 1, 0, 0]) Miscellaneous Lotka-Volterra predator-prey model The Lotka-Volterra model is well known for its application to predator-prey interactions but can also be used to model chemical, bio molecular and other auto regulating biological systems. Compartments are labelled Predator, pRey. The model.rate_function for prey reproduction, predator reproduction and predator death is defined as: function lotka_rf(output, parameters::Array{Float64, 1}, population::Array{Int64, 1}) # prey; predator reproduction; predator death output[1] = parameters[1] * population[2] output[2] = parameters[2] * population[1] * population[2] output[3] = parameters[3] * population[1] end with the transition matrix given as: m_transition = [ 0 1; 1 -1; -1 0 ] Ross-MacDonald malaria model The Ross-MacDonald malaria model is an extension of the Kermack-McKendrick SIS model which accounts for two species, human and mosquito. ADD CITATION. This example is simplified in two ways... SETIR model for bTB More information ADD list... [Compartmental models in epidemiology](https://en.wikipedia.org/wiki/Compartmental models in_epidemiology] (Wiki). References @article{kermack_contributions_1991, title = {Contributions to the mathematical theory of epidemics\u2014{I}}, volume = {53}, number = {1-2}, journal = {Bulletin of mathematical biology}, author = {Kermack, William O. and McKendrick, Anderson G.}, year = {1991}, pages = {33--55} } @article{pooley_using_2015, title = {Using model-based proposals for fast parameter inference on discrete state space, continuous-time {Markov} processes}, volume = {12}, issn = {1742-5689, 1742-5662}, url = {http://rsif.royalsocietypublishing.org/cgi/doi/10.1098/rsif.2015.0225}, doi = {10.1098/rsif.2015.0225}, language = {en}, number = {107}, urldate = {2017-02-20}, journal = {Journal of The Royal Society Interface}, author = {Pooley, C. M. and Bishop, S. C. and Marion, G.}, month = may, year = {2015}, pages = {20150225--20150225} } @article{berryman_orgins_1992, title = {The {Orgins} and {Evolution} of {Predator}-{Prey} {Theory}}, volume = {73}, issn = {00129658}, url = {http://doi.wiley.com/10.2307/1940005}, doi = {10.2307/1940005}, language = {en}, number = {5}, urldate = {2018-03-12}, journal = {Ecology}, author = {Berryman, Alan A.}, month = oct, year = {1992}, pages = {1530--1535} } @article{macdonald_analysis_1952, title = {The analysis of equilibrium in malaria}, volume = {49}, issn = {0041-3240}, language = {eng}, number = {9}, journal = {Tropical Diseases Bulletin}, author = {Macdonald, G.}, month = sep, year = {1952}, pmid = {12995455}, keywords = {Humans, Malaria, MALARIA}, pages = {813--829} }","title":"Models"},{"location":"models/#discuitjl-models","text":"This section describes the library of predefined models in Discuit. Models can also be user defined, or generated and modified according to need. The models included are mostly epidemiological (see Miscellaneous for other types of model). The next section described the model generating function and an overview of default model components.","title":"Discuit.jl models"},{"location":"models/#generating-models","text":"Pre defined models can be invoked by calling: generate_model(model_name, initial_condition, \u03c3 = 2.0) Examples model = generate_model( SIS , [100, 1]]); model = generate_model( SEIR , [100, 0, 1, 0]], 1.0)","title":"Generating models"},{"location":"models/#defaults","text":"Discuit.jl models are mutable struct s so it is convenient to generate pre defined models with default values for most things and overwrite as required. Only the model_name and initial_condition are required to call generate_model . The following gives an overview of important defaults that the user should note before proceeding with their analysis.","title":"Defaults"},{"location":"models/#observation-function","text":"The default model.obs_function returns a vector representing the state of the system at obsertion time $t_y$, i.e.: obs_fn(population::Array{Int64, 1}) = population","title":"Observation function"},{"location":"models/#observation-likelihood-model","text":"The default model.observation_model is Gaussian with observation error \u03c3 = 2.0 by default, which can be changed by providing an optional third parameter to generate_model per the example above, ADD latex.","title":"Observation likelihood model"},{"location":"models/#prior-density-function","text":"The default model.prior density function is weak, e.g. in a simple two parameter model it is equivalent to: function weak_prior(parameters::Array{Float64, 1}) parameters[1] 0.0 || return 0.0 parameters[2] 0.0 || return 0.0 return 1.0 end Note that all models generated by the ADD XREF have t0_index = 0 by default and that changing this parameter will likely require replacing the the density with something like: function weak_prior(parameters::Array{Float64, 1}) parameters[1] 0.0 || return 0.0 parameters[2] 0.0 || return 0.0 parameters[3] 0.0 || return 0.0 return 1.0 end where parameters[3] is designated as the, e.g. initial infection, which is assumed to have taken place prior to e.g. the initial observation at t = 0.0.","title":"Prior density function"},{"location":"models/#standard-kermack-mckendrick-models","text":"The standard Kermack-McKendrick SIR model can be used to model diseases which confer lasting immunity or situations where infected individuals have been detected and quarantined for treatment (Kermack and McKendrick, 1991). ADD REF: kermack contributions 1991 Discuit.generate_model( SI , [100, 1]) The susceptible-infectious ( \"SI\" ) is a very basic model with only one type of event. Individuals who become infected remain infected for the duration of trajectory: Discuit.generate_model( SIR , [100, 1, 0]) The classic Kermack-McKendrick susceptible-infectious-recovered ( \"SIR\" ) model includes an extra 'recovery' event and an additional compartment for recovered individuals: Discuit.generate_model( SIS , [100, 1]) The susceptible-infectious-susceptible ( \"SIS\" ) model is an extension of the classic Kermack-McKendrick SIR model for diseases which do not confer lasting immunity: Discuit.generate_model( SIRS , [100, 1, 0]) The susceptible-infectious-recovered-susceptible ( \"SIRS\" ) model incorporates all of the above, i.e. it is for diseases which do not confer long lasting immunity.","title":"Standard Kermack-McKendrick models"},{"location":"models/#latent-kermack-mckendrick-models","text":"The next class of models extend the classic Kermack-McKendrick by accounting for an exposed state E between infection and the onset of infectiousness. For example, the susceptible-exposed-infectious SEI model: Discuit.generate_model( SEI , [100, 1, 0]) The latent Kermack-McKendrick models can be extended in the same way, i.e, the SEIR model, SEIS and SEIRS : Discuit.generate_model( SEIR , [100, 1, 0, 0]) Discuit.generate_model( SEIS , [100, 1, 0]) Discuit.generate_model( SEIRS , [100, 1, 0, 0])","title":"Latent Kermack-McKendrick models"},{"location":"models/#miscellaneous","text":"","title":"Miscellaneous"},{"location":"models/#lotka-volterra-predator-prey-model","text":"The Lotka-Volterra model is well known for its application to predator-prey interactions but can also be used to model chemical, bio molecular and other auto regulating biological systems. Compartments are labelled Predator, pRey. The model.rate_function for prey reproduction, predator reproduction and predator death is defined as: function lotka_rf(output, parameters::Array{Float64, 1}, population::Array{Int64, 1}) # prey; predator reproduction; predator death output[1] = parameters[1] * population[2] output[2] = parameters[2] * population[1] * population[2] output[3] = parameters[3] * population[1] end with the transition matrix given as: m_transition = [ 0 1; 1 -1; -1 0 ]","title":"Lotka-Volterra predator-prey model"},{"location":"models/#ross-macdonald-malaria-model","text":"The Ross-MacDonald malaria model is an extension of the Kermack-McKendrick SIS model which accounts for two species, human and mosquito. ADD CITATION. This example is simplified in two ways...","title":"Ross-MacDonald malaria model"},{"location":"models/#setir-model-for-btb","text":"","title":"SETIR model for bTB"},{"location":"models/#more-information","text":"ADD list... [Compartmental models in epidemiology](https://en.wikipedia.org/wiki/Compartmental models in_epidemiology] (Wiki).","title":"More information"},{"location":"models/#references","text":"@article{kermack_contributions_1991, title = {Contributions to the mathematical theory of epidemics\u2014{I}}, volume = {53}, number = {1-2}, journal = {Bulletin of mathematical biology}, author = {Kermack, William O. and McKendrick, Anderson G.}, year = {1991}, pages = {33--55} } @article{pooley_using_2015, title = {Using model-based proposals for fast parameter inference on discrete state space, continuous-time {Markov} processes}, volume = {12}, issn = {1742-5689, 1742-5662}, url = {http://rsif.royalsocietypublishing.org/cgi/doi/10.1098/rsif.2015.0225}, doi = {10.1098/rsif.2015.0225}, language = {en}, number = {107}, urldate = {2017-02-20}, journal = {Journal of The Royal Society Interface}, author = {Pooley, C. M. and Bishop, S. C. and Marion, G.}, month = may, year = {2015}, pages = {20150225--20150225} } @article{berryman_orgins_1992, title = {The {Orgins} and {Evolution} of {Predator}-{Prey} {Theory}}, volume = {73}, issn = {00129658}, url = {http://doi.wiley.com/10.2307/1940005}, doi = {10.2307/1940005}, language = {en}, number = {5}, urldate = {2018-03-12}, journal = {Ecology}, author = {Berryman, Alan A.}, month = oct, year = {1992}, pages = {1530--1535} } @article{macdonald_analysis_1952, title = {The analysis of equilibrium in malaria}, volume = {49}, issn = {0041-3240}, language = {eng}, number = {9}, journal = {Tropical Diseases Bulletin}, author = {Macdonald, G.}, month = sep, year = {1952}, pmid = {12995455}, keywords = {Humans, Malaria, MALARIA}, pages = {813--829} }","title":"References"},{"location":"mcmc_intro/mcmc_intro/","text":"Introduction to MCMC Markov chain Monte Carlo Martin Burke, September 2018 Markov chain Monte Carlo (MCMC) methods... ADD BLURB This article assumes a basic familiarity with Monte Carlo methods, and in particular rejection sampling. See the introduction to Monte Carlo methods article for a basic overview. Metropolis-Hastings algorithm The Metropolis-Hastings algorithm is another method of obtaining samples from $F$. Like the basic rejection sampler it draws samples from a proposal distribution and accepts them with a given probability. However in this case the denominator of the acceptance probability equation is the previous sample $x_i$, which is also the basis for the next proposal. For example, in the simple rejection sampler we made proposals independently by drawing a random number between two and twelve. In this example we shall propose the new state $x_f$ by drawing a random number (which may be positive or negative) and adding it to $x_i$. # import some stuff for random number generation, analysis and plotting import numpy as np import pandas as pd from plotnine import * from plotnine.data import * # likelihood function def correct_likelihood(z): comb = 6 - abs(z - 7) return comb * 1/36 # likelihood estimator def dodgy_likelihood(z): like = correct_likelihood(z) pert = (np.random.random() - 0.5) * 0.1 return max(like + pert, 0) /usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88 return f(*args, **kwds) /usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88 return f(*args, **kwds) # metropolis hastings algorithm PROPOSAL = 8 # jump size def met_hastings_alg(steps, likelihood_function): #initialise xi = 2 lik_xi = likelihood_function(xi) lik_err = lik_xi - correct_likelihood(xi) markov_chain = list() num_accept = 0 # get some samples for x in range(1, steps + 1): # propose new state xf = xi + np.random.randint(-PROPOSAL, PROPOSAL+1) # evaluate likelihood lik_xf = likelihood_function(xf) # accept with mh probability accept = False mh_prop = lik_xf / lik_xi if mh_prop 1: # accept automatically accept = True else: # accept sometimes if np.random.random() mh_prop: accept = True # add sample to mc if accept: num_accept = num_accept + 1 rowi = (x, xi, lik_xi, xf, lik_xf, accept, num_accept / x, lik_err) markov_chain.append(rowi) # update xi if accept: xi = xf lik_xi = lik_xf lik_err = lik_xi - correct_likelihood(xi) # create data frame and return return pd.DataFrame(markov_chain, columns=[ i , xi , lik_xi , xf , lik_xf , accept , ar , lik_err ]) # run markov chain mc = met_hastings_alg(100000, dodgy_likelihood) print(mc) trace = ggplot(mc, aes(x = i , y = xi )) + geom_line() p = ggplot(mc, aes(x = xi , y = ..density.. )) + geom_histogram(binwidth=1) print(trace) i xi lik_xi xf lik_xf accept ar lik_err 0 1 2 0.018845 8 0.093632 True 1.000000 -0.008933 1 2 8 0.093632 6 0.171441 True 1.000000 -0.045257 2 3 6 0.171441 11 0.009422 False 0.666667 0.032552 3 4 6 0.171441 6 0.091026 False 0.500000 0.032552 4 5 6 0.171441 0 0.000000 False 0.400000 0.032552 5 6 6 0.171441 14 0.000000 False 0.333333 0.032552 6 7 6 0.171441 10 0.053377 True 0.428571 0.032552 7 8 10 0.053377 15 0.000000 False 0.375000 -0.029956 8 9 10 0.053377 8 0.179336 True 0.444444 -0.029956 9 10 8 0.179336 0 0.000000 False 0.400000 0.040447 10 11 8 0.179336 1 0.000000 False 0.363636 0.040447 11 12 8 0.179336 4 0.060740 False 0.333333 0.040447 12 13 8 0.179336 4 0.114071 True 0.384615 0.040447 13 14 4 0.114071 -2 0.000000 False 0.357143 0.030737 14 15 4 0.114071 1 0.004771 True 0.400000 0.030737 15 16 1 0.004771 -1 0.000000 False 0.375000 0.004771 16 17 1 0.004771 -1 0.000000 False 0.352941 0.004771 17 18 1 0.004771 0 0.000000 False 0.333333 0.004771 18 19 1 0.004771 7 0.191727 True 0.368421 0.004771 19 20 7 0.191727 10 0.077846 False 0.350000 0.025060 20 21 7 0.191727 12 0.052756 False 0.333333 0.025060 21 22 7 0.191727 7 0.157793 True 0.363636 0.025060 22 23 7 0.157793 6 0.106944 False 0.347826 -0.008873 23 24 7 0.157793 2 0.006802 True 0.375000 -0.008873 24 25 2 0.006802 -5 0.000000 False 0.360000 -0.020976 25 26 2 0.006802 -1 0.000000 False 0.346154 -0.020976 26 27 2 0.006802 0 0.000000 False 0.333333 -0.020976 27 28 2 0.006802 -1 0.000000 False 0.321429 -0.020976 28 29 2 0.006802 -6 0.000000 False 0.310345 -0.020976 29 30 2 0.006802 1 0.000000 False 0.300000 -0.020976 ... ... .. ... .. ... ... ... ... 99970 99971 9 0.159789 4 0.069303 False 0.440208 0.048677 99971 99972 9 0.159789 5 0.114957 True 0.440213 0.048677 99972 99973 5 0.114957 4 0.084242 False 0.440209 0.003846 99973 99974 5 0.114957 3 0.027291 True 0.440214 0.003846 99974 99975 3 0.027291 -3 0.000000 False 0.440210 -0.028265 99975 99976 3 0.027291 -2 0.000000 False 0.440206 -0.028265 99976 99977 3 0.027291 0 0.000000 False 0.440201 -0.028265 99977 99978 3 0.027291 11 0.073115 True 0.440207 -0.028265 99978 99979 11 0.073115 18 0.000000 False 0.440202 0.017560 99979 99980 11 0.073115 19 0.000000 False 0.440198 0.017560 99980 99981 11 0.073115 12 0.013020 False 0.440194 0.017560 99981 99982 11 0.073115 12 0.032099 False 0.440189 0.017560 99982 99983 11 0.073115 11 0.092939 True 0.440195 0.017560 99983 99984 11 0.092939 6 0.102638 True 0.440200 0.037384 99984 99985 6 0.102638 5 0.127289 True 0.440206 -0.036251 99985 99986 5 0.127289 1 0.000000 False 0.440202 0.016178 99986 99987 5 0.127289 11 0.066883 True 0.440207 0.016178 99987 99988 11 0.066883 17 0.000000 False 0.440203 0.011328 99988 99989 11 0.066883 10 0.125619 True 0.440208 0.011328 99989 99990 10 0.125619 2 0.007658 False 0.440204 0.042286 99990 99991 10 0.125619 15 0.000000 False 0.440200 0.042286 99991 99992 10 0.125619 6 0.098521 True 0.440205 0.042286 99992 99993 6 0.098521 10 0.069860 False 0.440201 -0.040368 99993 99994 6 0.098521 3 0.026114 False 0.440196 -0.040368 99994 99995 6 0.098521 13 0.000000 False 0.440192 -0.040368 99995 99996 6 0.098521 1 0.049607 True 0.440198 -0.040368 99996 99997 1 0.049607 4 0.092572 True 0.440203 0.049607 99997 99998 4 0.092572 3 0.074566 True 0.440209 0.009239 99998 99999 3 0.074566 -2 0.000000 False 0.440204 0.019011 99999 100000 3 0.074566 -2 0.000000 False 0.440200 0.019011 [100000 rows x 8 columns] ggplot: (8731147382536)","title":"Mcmc intro"},{"location":"mcmc_intro/mcmc_intro/#introduction-to-mcmc","text":"","title":"Introduction to MCMC"},{"location":"mcmc_intro/mcmc_intro/#markov-chain-monte-carlo","text":"","title":"Markov chain Monte Carlo"},{"location":"mcmc_intro/mcmc_intro/#martin-burke-september-2018","text":"Markov chain Monte Carlo (MCMC) methods... ADD BLURB This article assumes a basic familiarity with Monte Carlo methods, and in particular rejection sampling. See the introduction to Monte Carlo methods article for a basic overview.","title":"Martin Burke, September 2018"},{"location":"mcmc_intro/mcmc_intro/#metropolis-hastings-algorithm","text":"The Metropolis-Hastings algorithm is another method of obtaining samples from $F$. Like the basic rejection sampler it draws samples from a proposal distribution and accepts them with a given probability. However in this case the denominator of the acceptance probability equation is the previous sample $x_i$, which is also the basis for the next proposal. For example, in the simple rejection sampler we made proposals independently by drawing a random number between two and twelve. In this example we shall propose the new state $x_f$ by drawing a random number (which may be positive or negative) and adding it to $x_i$. # import some stuff for random number generation, analysis and plotting import numpy as np import pandas as pd from plotnine import * from plotnine.data import * # likelihood function def correct_likelihood(z): comb = 6 - abs(z - 7) return comb * 1/36 # likelihood estimator def dodgy_likelihood(z): like = correct_likelihood(z) pert = (np.random.random() - 0.5) * 0.1 return max(like + pert, 0) /usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88 return f(*args, **kwds) /usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88 return f(*args, **kwds) # metropolis hastings algorithm PROPOSAL = 8 # jump size def met_hastings_alg(steps, likelihood_function): #initialise xi = 2 lik_xi = likelihood_function(xi) lik_err = lik_xi - correct_likelihood(xi) markov_chain = list() num_accept = 0 # get some samples for x in range(1, steps + 1): # propose new state xf = xi + np.random.randint(-PROPOSAL, PROPOSAL+1) # evaluate likelihood lik_xf = likelihood_function(xf) # accept with mh probability accept = False mh_prop = lik_xf / lik_xi if mh_prop 1: # accept automatically accept = True else: # accept sometimes if np.random.random() mh_prop: accept = True # add sample to mc if accept: num_accept = num_accept + 1 rowi = (x, xi, lik_xi, xf, lik_xf, accept, num_accept / x, lik_err) markov_chain.append(rowi) # update xi if accept: xi = xf lik_xi = lik_xf lik_err = lik_xi - correct_likelihood(xi) # create data frame and return return pd.DataFrame(markov_chain, columns=[ i , xi , lik_xi , xf , lik_xf , accept , ar , lik_err ]) # run markov chain mc = met_hastings_alg(100000, dodgy_likelihood) print(mc) trace = ggplot(mc, aes(x = i , y = xi )) + geom_line() p = ggplot(mc, aes(x = xi , y = ..density.. )) + geom_histogram(binwidth=1) print(trace) i xi lik_xi xf lik_xf accept ar lik_err 0 1 2 0.018845 8 0.093632 True 1.000000 -0.008933 1 2 8 0.093632 6 0.171441 True 1.000000 -0.045257 2 3 6 0.171441 11 0.009422 False 0.666667 0.032552 3 4 6 0.171441 6 0.091026 False 0.500000 0.032552 4 5 6 0.171441 0 0.000000 False 0.400000 0.032552 5 6 6 0.171441 14 0.000000 False 0.333333 0.032552 6 7 6 0.171441 10 0.053377 True 0.428571 0.032552 7 8 10 0.053377 15 0.000000 False 0.375000 -0.029956 8 9 10 0.053377 8 0.179336 True 0.444444 -0.029956 9 10 8 0.179336 0 0.000000 False 0.400000 0.040447 10 11 8 0.179336 1 0.000000 False 0.363636 0.040447 11 12 8 0.179336 4 0.060740 False 0.333333 0.040447 12 13 8 0.179336 4 0.114071 True 0.384615 0.040447 13 14 4 0.114071 -2 0.000000 False 0.357143 0.030737 14 15 4 0.114071 1 0.004771 True 0.400000 0.030737 15 16 1 0.004771 -1 0.000000 False 0.375000 0.004771 16 17 1 0.004771 -1 0.000000 False 0.352941 0.004771 17 18 1 0.004771 0 0.000000 False 0.333333 0.004771 18 19 1 0.004771 7 0.191727 True 0.368421 0.004771 19 20 7 0.191727 10 0.077846 False 0.350000 0.025060 20 21 7 0.191727 12 0.052756 False 0.333333 0.025060 21 22 7 0.191727 7 0.157793 True 0.363636 0.025060 22 23 7 0.157793 6 0.106944 False 0.347826 -0.008873 23 24 7 0.157793 2 0.006802 True 0.375000 -0.008873 24 25 2 0.006802 -5 0.000000 False 0.360000 -0.020976 25 26 2 0.006802 -1 0.000000 False 0.346154 -0.020976 26 27 2 0.006802 0 0.000000 False 0.333333 -0.020976 27 28 2 0.006802 -1 0.000000 False 0.321429 -0.020976 28 29 2 0.006802 -6 0.000000 False 0.310345 -0.020976 29 30 2 0.006802 1 0.000000 False 0.300000 -0.020976 ... ... .. ... .. ... ... ... ... 99970 99971 9 0.159789 4 0.069303 False 0.440208 0.048677 99971 99972 9 0.159789 5 0.114957 True 0.440213 0.048677 99972 99973 5 0.114957 4 0.084242 False 0.440209 0.003846 99973 99974 5 0.114957 3 0.027291 True 0.440214 0.003846 99974 99975 3 0.027291 -3 0.000000 False 0.440210 -0.028265 99975 99976 3 0.027291 -2 0.000000 False 0.440206 -0.028265 99976 99977 3 0.027291 0 0.000000 False 0.440201 -0.028265 99977 99978 3 0.027291 11 0.073115 True 0.440207 -0.028265 99978 99979 11 0.073115 18 0.000000 False 0.440202 0.017560 99979 99980 11 0.073115 19 0.000000 False 0.440198 0.017560 99980 99981 11 0.073115 12 0.013020 False 0.440194 0.017560 99981 99982 11 0.073115 12 0.032099 False 0.440189 0.017560 99982 99983 11 0.073115 11 0.092939 True 0.440195 0.017560 99983 99984 11 0.092939 6 0.102638 True 0.440200 0.037384 99984 99985 6 0.102638 5 0.127289 True 0.440206 -0.036251 99985 99986 5 0.127289 1 0.000000 False 0.440202 0.016178 99986 99987 5 0.127289 11 0.066883 True 0.440207 0.016178 99987 99988 11 0.066883 17 0.000000 False 0.440203 0.011328 99988 99989 11 0.066883 10 0.125619 True 0.440208 0.011328 99989 99990 10 0.125619 2 0.007658 False 0.440204 0.042286 99990 99991 10 0.125619 15 0.000000 False 0.440200 0.042286 99991 99992 10 0.125619 6 0.098521 True 0.440205 0.042286 99992 99993 6 0.098521 10 0.069860 False 0.440201 -0.040368 99993 99994 6 0.098521 3 0.026114 False 0.440196 -0.040368 99994 99995 6 0.098521 13 0.000000 False 0.440192 -0.040368 99995 99996 6 0.098521 1 0.049607 True 0.440198 -0.040368 99996 99997 1 0.049607 4 0.092572 True 0.440203 0.049607 99997 99998 4 0.092572 3 0.074566 True 0.440209 0.009239 99998 99999 3 0.074566 -2 0.000000 False 0.440204 0.019011 99999 100000 3 0.074566 -2 0.000000 False 0.440200 0.019011 [100000 rows x 8 columns] ggplot: (8731147382536)","title":"Metropolis-Hastings algorithm"},{"location":"monte_carlo_intro/monte_carlo_intro/","text":"Introduction to Monte Carlo methods Introduction Sampling from discrete probability distributions Martin Burke, August 2018 Monte Carlo methods are a way of drawing samples from probability distributions, as well as being useful for problems such as optimization and computing integrals). In the examples below the (not so) difficult to sample distribution of interest is $X$: the outcome in a game of dice where the score $x = d_1 + d_2$ is the sum of two fair dice and the likelihood of any given score is denoted $f(x)$. UPDATED. Different sampling algorithms can be understood in terms of the challenges and requirements they are designed to accomodate. In some situations we may know enough about the system to draw directly (i.e. simulate) from $X$. In others we may be restricted to computing the probability mass function (PMF) $f(x)$; an unbiased approximation $\\hat{f}(x)$; or some quantity proportional to the likelihood $q \\propto f(x)$. We may also be interested in specific regions or scenarios concerning the target distribution such as the likelihood rolling 10 or more. More formally, we might wish to evaluate $\\int_{10}^{12} f(x) dx$. Problem Algorithms Draw directly from $X$ Plain Monte Carlo Can only compute $f(x)$, $\\hat Rejection, Importance sampling Need to compute $\\int_ Importance sampling Plain Monte Carlo The first sampler is for situations where we know enough about the target distribution to draw samples directly. To do this we use our knowledge of the data generating process to define a function which draws two random numbers distributed uniformly from one to six and returns the sum: # import some stuff for random number generation, analysis and plotting import numpy as np import pandas as pd from plotnine import * from plotnine.data import * # draw directly from X def dat_gen_function(): x = np.random.randint(1, 7) y = np.random.randint(1, 7) return x + y /usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88 return f(*args, **kwds) /usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88 return f(*args, **kwds) The algorithm itself is a simple one: it iteratively calls the function (passed as a parameter 'f') for a given number of steps. Each sample is appended to a list which is returned as a pandas data frame. # example 1) monte carlo def plain_monte_carlo(steps, f): # get some samples samples = list() for i in range(0, steps): x = f() # create a tuple and append to list sample = (i, x) samples.append(sample) # create data frame and return return pd.DataFrame(samples, columns=[ i , x ]) We can now run the algorithm and plot the results. s = plain_monte_carlo(200, dat_gen_function) p = ggplot(s, aes( x )) + geom_histogram(binwidth=1) print(p) ggplot: (-9223363272471025264) Note that as the number of steps is increased the random noise in the distribution of samples obtained is reduced. That is, they converge upon the true target distribution: $X$. Rejection sampling In situations where sampling from $X$ directly is difficult we may still be able to obtain samples if we are able to compute the PMF of the target distribution: $f(x)$ or an unbiased estimate. The rejection sampling method accomplishes this by drawing $x$ from some other easier to sample proposal distribution $G$ and accepting or rejecting the sample with probability: $pr(accept) = \\frac{f(x)}{C g(x)}$ where C is a constant chosen such that $f(x) C g(x)$ for all $x$. Naturally since C is a constant this method works equally well in situations where it is more convenient to compute a quantity proportional to the likelihood. We will therefore begin by defining a function which returns the likelihood multiplied by an arbitrary \"unknown\" constant: $q(x)$: # arbitrary constant UC = 2 # proportional quantity likelihood function def function_q_x(x): comb = 6 - abs(x - 7) return comb * 1/36 * UC The algorithm is similar to the first but this time a likelihood function (or in this case $q(x)$) is passed as the function parameter. The proposal distribution $G$ is uniform between two and twelve. This is computationally convenient in the sense that we only have to evaluate $g(x)$ once but also somewhat wasteful in that many proposals are rejected. The choice of proposal distribution is therefore an important factor in algorithm efficiency but not one that we shall explore in detail here. # example 2) rejection sampler def rejection_sampler(steps, likelihood_function): # compute C * g(x) m_gx = 4 * 1/11 # C is chosen such that g(x) always q(x) # get some samples samples = list() for i in range(1, steps + 1): # draw from proposal dist (uniform ~ 2, 12) x = np.random.randint(2, 13) # compute acceptance probability pr_a = likelihood_function(x) / m_gx # accept (or not) if np.random.random() pr_a: sample = (i, x) samples.append(sample) # return as pandas data frame return pd.DataFrame(samples, columns=[ i , x ]) Note that in contrast to the previous example each iteration of the algorithm involves two probabistic steps: the proposal itself and the additional step of accepting or rejecting with the computed probability. s = rejection_sampler(10000, function_q_x) p = ggplot(s, aes( x )) + geom_histogram(binwidth=1) print(p) ggplot: (8764383717959) NEED TO ADD acceptance rate commentary. Importance sampling The first two examples provided methods for drawing samples from $X$ in different situations. Importance sampling algorithms by contrast are designed to draw samples from the proposal density $G$ and weight them so as to recover information about $X$. First we consider the basic concept and then apply it to a slightly more difficult problem to understand why this might sometimes be useful. Basic example The proposal density used here is uniform from one to thirteen (i.e. we shall assume that we are not particularly good at choosing proposals): # 3.1) importance sampling: basic concept def simple_importance_sampler(steps, likelihood_function): # get some samples samples = list() for i in range(1, steps + 1): # draw from proposal dist (uniform ~ 1, 13) x = np.random.randint(1, 14) # weight sample by target dist # (since the g(x) is uniform we disregard for now) w = likelihood_function(x) sample = (i, x, w) samples.append(sample) # create data frame and return return pd.DataFrame(samples, columns=[ i , x , w ]) We can now run the sampler using the likelihood function we already defined: s = simple_importance_sampler(10000, function_q_x) p = ggplot(s, aes(x = x , y = ..density.. , weight = w )) + geom_histogram(binwidth=1) print(p) ggplot: (-9223363272473221032) Payoff example We now consider a slightly more complex problem to better illustrate the usefulness of importance sampling. Let $h(x)$ be a function that defines the payoff in a game of dice in a pretend casino. The player wins by rolling nine or more with the winnings being double the stake on nine; triple on ten and so on. The house wins if the player rolls seven or less and eight is a draw, i.e.the player keeps their stake. # payoff function def function_h_x(x): if x 8: return 0 else: return x - 7 We are now interested in finding the expected value of the pay off. More formally, we wish to evaluate the following integral: $\\int_{8}^{12} h(x) f(x) dx$ We shall also assume that we are in a position to compute an unbiased estimate of the full likelihood $\\hat{f}(x)$: # likelihood function (not directly used) def correct_likelihood(x): comb = 6 - abs(x - 7) return comb * 1/36 # likelihood estimator def dodgy_likelihood(x): like = correct_likelihood(x) pert = (np.random.random() - 0.5) * 0.1 return max(like + pert, 0) Note the updated weight calculation which accounts for the proposal density in addition to the estimated likelihood and the pay off function: # 3.2) importance sampling: pay off example def payoff_importance_sampler(steps, p1, p2, likelihood_function, pay_off_fn): # get some samples samples = list() for i in range(1, steps + 1): # draw from proposal dist (uniform ~ p1, p2) x = np.random.randint(p1, p2 + 1) # weight sample by f(x) / g(x) * h(x) w = likelihood_function(x) * (p2 - p1 + 1) * pay_off_fn(x) sample = (i, x, w) samples.append(sample) # create data frame and return return pd.DataFrame(samples, columns=[ i , x , w ]) Note that the proposal distribution $G$ is still uniform but is now parameterised. We begin by sampling using a better (but still not very good) proposal distribution, uniform on the range of possible outcomes. s = payoff_importance_sampler(1000, 2, 12, dodgy_likelihood, function_h_x) ev = s[ w ].mean() print( expected payoff: {} .format(ev)) p = ggplot(s, aes(x = x , y = ..density.. , weight = w )) + geom_histogram(binwidth=1) print(p) expected payoff: 0.9973450094506978 ggplot: (8764381554717) Running the algorithm we notice that samples obtained for proposals less than eight are essentially wasted since they do not contribute to the information we are able to recover about the expected payoff. We can therefore change the proposal density to only select from the desired range in order to improve the efficiency of the algorithm: s = payoff_importance_sampler(1000, 8, 12, dodgy_likelihood, function_h_x) ev = s[ w ].mean() print( expected payoff: {} .format(ev)) p = ggplot(s, aes(x = x , y = ..density.. , weight = w )) + geom_histogram(binwidth=1) print(p) expected payoff: 0.9997709937321856 ggplot: (8764381548990) We can also test the performace gain by computing the average error overone hundred runs for a given number of samples from each proposal density: true_payoff = 35/36 results = list() for i in range(0, 100): s1 = payoff_importance_sampler(1000, 2, 12, dodgy_likelihood, function_h_x) err1 = abs(true_payoff - s1[ w ].mean()) s2 = payoff_importance_sampler(1000, 8, 12, dodgy_likelihood, function_h_x) err2 = abs(true_payoff - s2[ w ].mean()) results.append((err1, err2)) results = pd.DataFrame(results, columns=[ err1 , err2 ]) print( First proposal average error: {} .format(results[ err1 ].mean())) print( Second proposal average error: {} .format(results[ err2 ].mean())) First proposal average error: 0.038710788275420495 Second proposal average error: 0.016279646898299353 Unsurprisingly the average error is lower for the narrower proposal range. This demonstrates that importance sampling is a useful option for exploiring specific regions of the target distribution. This is a useful property with a range of applications from financial risk models to predicting the frequency of rare events in climate models. Summary This document introduced three basic classes of Monte Carlo algorithm in the context of drawing (independent) samples from a discrete probability distribution and touched upon the related problem of solving integrals. The next notebook introduces a more advanced class of Monte Carlo methods which draw future samples based on the current sample (i.e. dependent) to form a Markov chain (MCMC).","title":"Monte carlo intro"},{"location":"monte_carlo_intro/monte_carlo_intro/#introduction-to-monte-carlo-methods","text":"","title":"Introduction to Monte Carlo methods"},{"location":"monte_carlo_intro/monte_carlo_intro/#introduction","text":"","title":"Introduction"},{"location":"monte_carlo_intro/monte_carlo_intro/#sampling-from-discrete-probability-distributions","text":"","title":"Sampling from discrete probability distributions"},{"location":"monte_carlo_intro/monte_carlo_intro/#martin-burke-august-2018","text":"Monte Carlo methods are a way of drawing samples from probability distributions, as well as being useful for problems such as optimization and computing integrals). In the examples below the (not so) difficult to sample distribution of interest is $X$: the outcome in a game of dice where the score $x = d_1 + d_2$ is the sum of two fair dice and the likelihood of any given score is denoted $f(x)$. UPDATED. Different sampling algorithms can be understood in terms of the challenges and requirements they are designed to accomodate. In some situations we may know enough about the system to draw directly (i.e. simulate) from $X$. In others we may be restricted to computing the probability mass function (PMF) $f(x)$; an unbiased approximation $\\hat{f}(x)$; or some quantity proportional to the likelihood $q \\propto f(x)$. We may also be interested in specific regions or scenarios concerning the target distribution such as the likelihood rolling 10 or more. More formally, we might wish to evaluate $\\int_{10}^{12} f(x) dx$. Problem Algorithms Draw directly from $X$ Plain Monte Carlo Can only compute $f(x)$, $\\hat Rejection, Importance sampling Need to compute $\\int_ Importance sampling","title":"Martin Burke, August 2018"},{"location":"monte_carlo_intro/monte_carlo_intro/#plain-monte-carlo","text":"The first sampler is for situations where we know enough about the target distribution to draw samples directly. To do this we use our knowledge of the data generating process to define a function which draws two random numbers distributed uniformly from one to six and returns the sum: # import some stuff for random number generation, analysis and plotting import numpy as np import pandas as pd from plotnine import * from plotnine.data import * # draw directly from X def dat_gen_function(): x = np.random.randint(1, 7) y = np.random.randint(1, 7) return x + y /usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88 return f(*args, **kwds) /usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88 return f(*args, **kwds) The algorithm itself is a simple one: it iteratively calls the function (passed as a parameter 'f') for a given number of steps. Each sample is appended to a list which is returned as a pandas data frame. # example 1) monte carlo def plain_monte_carlo(steps, f): # get some samples samples = list() for i in range(0, steps): x = f() # create a tuple and append to list sample = (i, x) samples.append(sample) # create data frame and return return pd.DataFrame(samples, columns=[ i , x ]) We can now run the algorithm and plot the results. s = plain_monte_carlo(200, dat_gen_function) p = ggplot(s, aes( x )) + geom_histogram(binwidth=1) print(p) ggplot: (-9223363272471025264) Note that as the number of steps is increased the random noise in the distribution of samples obtained is reduced. That is, they converge upon the true target distribution: $X$.","title":"Plain Monte Carlo"},{"location":"monte_carlo_intro/monte_carlo_intro/#rejection-sampling","text":"In situations where sampling from $X$ directly is difficult we may still be able to obtain samples if we are able to compute the PMF of the target distribution: $f(x)$ or an unbiased estimate. The rejection sampling method accomplishes this by drawing $x$ from some other easier to sample proposal distribution $G$ and accepting or rejecting the sample with probability: $pr(accept) = \\frac{f(x)}{C g(x)}$ where C is a constant chosen such that $f(x) C g(x)$ for all $x$. Naturally since C is a constant this method works equally well in situations where it is more convenient to compute a quantity proportional to the likelihood. We will therefore begin by defining a function which returns the likelihood multiplied by an arbitrary \"unknown\" constant: $q(x)$: # arbitrary constant UC = 2 # proportional quantity likelihood function def function_q_x(x): comb = 6 - abs(x - 7) return comb * 1/36 * UC The algorithm is similar to the first but this time a likelihood function (or in this case $q(x)$) is passed as the function parameter. The proposal distribution $G$ is uniform between two and twelve. This is computationally convenient in the sense that we only have to evaluate $g(x)$ once but also somewhat wasteful in that many proposals are rejected. The choice of proposal distribution is therefore an important factor in algorithm efficiency but not one that we shall explore in detail here. # example 2) rejection sampler def rejection_sampler(steps, likelihood_function): # compute C * g(x) m_gx = 4 * 1/11 # C is chosen such that g(x) always q(x) # get some samples samples = list() for i in range(1, steps + 1): # draw from proposal dist (uniform ~ 2, 12) x = np.random.randint(2, 13) # compute acceptance probability pr_a = likelihood_function(x) / m_gx # accept (or not) if np.random.random() pr_a: sample = (i, x) samples.append(sample) # return as pandas data frame return pd.DataFrame(samples, columns=[ i , x ]) Note that in contrast to the previous example each iteration of the algorithm involves two probabistic steps: the proposal itself and the additional step of accepting or rejecting with the computed probability. s = rejection_sampler(10000, function_q_x) p = ggplot(s, aes( x )) + geom_histogram(binwidth=1) print(p) ggplot: (8764383717959) NEED TO ADD acceptance rate commentary.","title":"Rejection sampling"},{"location":"monte_carlo_intro/monte_carlo_intro/#importance-sampling","text":"The first two examples provided methods for drawing samples from $X$ in different situations. Importance sampling algorithms by contrast are designed to draw samples from the proposal density $G$ and weight them so as to recover information about $X$. First we consider the basic concept and then apply it to a slightly more difficult problem to understand why this might sometimes be useful.","title":"Importance sampling"},{"location":"monte_carlo_intro/monte_carlo_intro/#basic-example","text":"The proposal density used here is uniform from one to thirteen (i.e. we shall assume that we are not particularly good at choosing proposals): # 3.1) importance sampling: basic concept def simple_importance_sampler(steps, likelihood_function): # get some samples samples = list() for i in range(1, steps + 1): # draw from proposal dist (uniform ~ 1, 13) x = np.random.randint(1, 14) # weight sample by target dist # (since the g(x) is uniform we disregard for now) w = likelihood_function(x) sample = (i, x, w) samples.append(sample) # create data frame and return return pd.DataFrame(samples, columns=[ i , x , w ]) We can now run the sampler using the likelihood function we already defined: s = simple_importance_sampler(10000, function_q_x) p = ggplot(s, aes(x = x , y = ..density.. , weight = w )) + geom_histogram(binwidth=1) print(p) ggplot: (-9223363272473221032)","title":"Basic example"},{"location":"monte_carlo_intro/monte_carlo_intro/#payoff-example","text":"We now consider a slightly more complex problem to better illustrate the usefulness of importance sampling. Let $h(x)$ be a function that defines the payoff in a game of dice in a pretend casino. The player wins by rolling nine or more with the winnings being double the stake on nine; triple on ten and so on. The house wins if the player rolls seven or less and eight is a draw, i.e.the player keeps their stake. # payoff function def function_h_x(x): if x 8: return 0 else: return x - 7 We are now interested in finding the expected value of the pay off. More formally, we wish to evaluate the following integral: $\\int_{8}^{12} h(x) f(x) dx$ We shall also assume that we are in a position to compute an unbiased estimate of the full likelihood $\\hat{f}(x)$: # likelihood function (not directly used) def correct_likelihood(x): comb = 6 - abs(x - 7) return comb * 1/36 # likelihood estimator def dodgy_likelihood(x): like = correct_likelihood(x) pert = (np.random.random() - 0.5) * 0.1 return max(like + pert, 0) Note the updated weight calculation which accounts for the proposal density in addition to the estimated likelihood and the pay off function: # 3.2) importance sampling: pay off example def payoff_importance_sampler(steps, p1, p2, likelihood_function, pay_off_fn): # get some samples samples = list() for i in range(1, steps + 1): # draw from proposal dist (uniform ~ p1, p2) x = np.random.randint(p1, p2 + 1) # weight sample by f(x) / g(x) * h(x) w = likelihood_function(x) * (p2 - p1 + 1) * pay_off_fn(x) sample = (i, x, w) samples.append(sample) # create data frame and return return pd.DataFrame(samples, columns=[ i , x , w ]) Note that the proposal distribution $G$ is still uniform but is now parameterised. We begin by sampling using a better (but still not very good) proposal distribution, uniform on the range of possible outcomes. s = payoff_importance_sampler(1000, 2, 12, dodgy_likelihood, function_h_x) ev = s[ w ].mean() print( expected payoff: {} .format(ev)) p = ggplot(s, aes(x = x , y = ..density.. , weight = w )) + geom_histogram(binwidth=1) print(p) expected payoff: 0.9973450094506978 ggplot: (8764381554717) Running the algorithm we notice that samples obtained for proposals less than eight are essentially wasted since they do not contribute to the information we are able to recover about the expected payoff. We can therefore change the proposal density to only select from the desired range in order to improve the efficiency of the algorithm: s = payoff_importance_sampler(1000, 8, 12, dodgy_likelihood, function_h_x) ev = s[ w ].mean() print( expected payoff: {} .format(ev)) p = ggplot(s, aes(x = x , y = ..density.. , weight = w )) + geom_histogram(binwidth=1) print(p) expected payoff: 0.9997709937321856 ggplot: (8764381548990) We can also test the performace gain by computing the average error overone hundred runs for a given number of samples from each proposal density: true_payoff = 35/36 results = list() for i in range(0, 100): s1 = payoff_importance_sampler(1000, 2, 12, dodgy_likelihood, function_h_x) err1 = abs(true_payoff - s1[ w ].mean()) s2 = payoff_importance_sampler(1000, 8, 12, dodgy_likelihood, function_h_x) err2 = abs(true_payoff - s2[ w ].mean()) results.append((err1, err2)) results = pd.DataFrame(results, columns=[ err1 , err2 ]) print( First proposal average error: {} .format(results[ err1 ].mean())) print( Second proposal average error: {} .format(results[ err2 ].mean())) First proposal average error: 0.038710788275420495 Second proposal average error: 0.016279646898299353 Unsurprisingly the average error is lower for the narrower proposal range. This demonstrates that importance sampling is a useful option for exploiring specific regions of the target distribution. This is a useful property with a range of applications from financial risk models to predicting the frequency of rare events in climate models.","title":"Payoff example"},{"location":"monte_carlo_intro/monte_carlo_intro/#summary","text":"This document introduced three basic classes of Monte Carlo algorithm in the context of drawing (independent) samples from a discrete probability distribution and touched upon the related problem of solving integrals. The next notebook introduces a more advanced class of Monte Carlo methods which draw future samples based on the current sample (i.e. dependent) to form a Markov chain (MCMC).","title":"Summary"}]}